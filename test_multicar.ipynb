{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy.linalg as LA\n",
    "import random\n",
    "import pandas as pd\n",
    "from numpy import inf\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "import pyprind\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero zero\n",
      "zero zero\n",
      "zero\n",
      "zero [] 1048576\n",
      "zero\n",
      "zero [] 1048576\n",
      "zero\n"
     ]
    }
   ],
   "source": [
    "############################################### DO NOT TOUCH THIS SNIPPET ############################################### \n",
    "df1 = pd.read_csv('data/trajectories-0400-0415.csv')\n",
    "all_index=df1.index.to_numpy()\n",
    "last_row=list(np.zeros(18))\n",
    "df1.loc[len(all_index)]=last_row\n",
    "df1.loc[len(all_index)+1]=last_row\n",
    "df1.loc[len(all_index)+2]=last_row\n",
    "df2 = pd.read_csv('data/trajectories-0500-0515.csv')\n",
    "df3 = pd.read_csv('data/trajectories-0515-0530.csv')\n",
    "\n",
    "## Adding two empty columns for velocities ##\n",
    "df1['V_X']=np.nan\n",
    "df1['V_Y']=np.nan\n",
    "\n",
    "lane=7\n",
    "filtered_df=df1.loc[df1['Lane_ID']==lane]\n",
    "list_mergingvehicles=list(filtered_df.Vehicle_ID.unique())\n",
    "df1_merging_outliers= [5,210,376,388,409,458,465,743,1005,1097,1133,1215,1408,1724,1812,1909,2387,2438,1778,2911]\n",
    "\n",
    "#print(list_mergingvehicles)\n",
    "\n",
    "\n",
    "####### Filtering data #########\n",
    "    ####### AND ####### \n",
    "#### Calculating Velocities ####\n",
    "dict_interacting_vehicles={}\n",
    "vehicles_actually_merging=[]\n",
    "vehicle_row_indices={}\n",
    "for v_ego in list_mergingvehicles:\n",
    "    if v_ego not in df1_merging_outliers:\n",
    "        \n",
    "        ## Extra data after merging ##\n",
    "        extra_time_steps=10\n",
    "        \n",
    "        ## Finding interacting cars ## \n",
    "        ## For now assume that only cars following and preceeding right after it has merged\n",
    "        ## the ego vehicle interact with the ego vehicle\n",
    "        df_ego=df1.loc[df1['Vehicle_ID']==v_ego]\n",
    "        change_index=df_ego.loc[df_ego['Lane_ID']==6].index.to_numpy()[0] # The index at which lane has changed to 6\n",
    "        change_Glob_time=int(df1.iloc[change_index]['Global_Time'])\n",
    "        v_preced_ego=int(df1.iloc[change_index]['Preceding'])\n",
    "        v_follow_ego=int(df1.iloc[change_index]['Following'])\n",
    "        dict_interacting_vehicles[str(int(v_ego))]=[v_preced_ego,v_ego,v_follow_ego]\n",
    "        #v_follow_ego_follow=int(df1.loc[(df1['Vehicle_ID']==v_follow_ego) & (df1['Global_Time']==change_Glob_time)]['Following'])    \n",
    "        #v_preced_ego_preced=int(df1.loc[(df1['Vehicle_ID']==v_preced_ego) & (df1['Global_Time']==change_Glob_time)]['Preceding'])\n",
    "        #dict_interacting_vehicles[str(v_ego)]=[v_preced_ego_preced,v_preced_ego,v_ego,v_follow_ego,v_follow_ego_follow]\n",
    "\n",
    "        ## Indices for ego vehicle\n",
    "        vehicles_actually_merging.append(v_ego)\n",
    "        temp=df1.loc[(df1['Vehicle_ID']==v_ego) & (df1['Lane_ID']==7)]\n",
    "        data_index_required_E=list(temp.loc[temp['Vehicle_ID']==v_ego].index.to_numpy())             \n",
    "\n",
    "        ############################################################\n",
    "        ## Indices for vehicle preceding ego - Lets call this VPE ##\n",
    "        temp_v_preced_ego=df1.loc[df1['Vehicle_ID']==v_preced_ego]\n",
    "        index_v_preced_ego_at_lchange=temp_v_preced_ego.loc[temp_v_preced_ego['Global_Time']==change_Glob_time].index.to_numpy()\n",
    "        #print(index_v_preced_ego_at_lchange)\n",
    "        if v_preced_ego!=0:            \n",
    "            if len(index_v_preced_ego_at_lchange)!=0:\n",
    "                index_VPE_lchange=index_v_preced_ego_at_lchange[0]        \n",
    "            else:\n",
    "                # This is when VPE is identified but there is no data for it\n",
    "                index_VPE_lchange=len(all_index)+1\n",
    "                print(\"zero\",index_v_preced_ego_at_lchange,index_VPE_lchange)        \n",
    "        \n",
    "        else:\n",
    "            # When ther is no vehicle i.e VPE = 0\n",
    "            index_VPE_lchange=len(all_index)+1\n",
    "            print(\"zero zero\")\n",
    "        \n",
    "        \n",
    "        data_index_required_VPE=[]\n",
    "        for i in range(len(data_index_required_E)):\n",
    "            if index_VPE_lchange==len(all_index)+1:\n",
    "                data_index_required_VPE.append(index_VPE_lchange)    \n",
    "            else:\n",
    "                data_index_required_VPE.append(index_VPE_lchange-i-1)\n",
    "        \n",
    "        #print(data_index_required_VPE)\n",
    "        \n",
    "        temp_extra_time=[]\n",
    "        \n",
    "        for i in range(extra_time_steps):\n",
    "            if index_VPE_lchange==len(all_index)+1:\n",
    "                temp_extra_time.append(index_VPE_lchange)    \n",
    "            else:\n",
    "                temp_extra_time.append(index_VPE_lchange+i)\n",
    "\n",
    "        data_index_required_VPE.extend(temp_extra_time)\n",
    "        data_index_required_VPE.sort()\n",
    "        \n",
    "        ############################################################\n",
    "        \n",
    "        ############################################################\n",
    "        ## Indices for vehicle following ego - Lets call this VFE ##\n",
    "        temp_v_follow_ego=df1.loc[df1['Vehicle_ID']==v_follow_ego]\n",
    "        index_v_follow_ego_at_lchange=temp_v_follow_ego.loc[temp_v_follow_ego['Global_Time']==change_Glob_time].index.to_numpy()\n",
    "        #print(index_v_preced_ego_at_lchange)\n",
    "        if v_follow_ego!=0:            \n",
    "            if len(index_v_follow_ego_at_lchange)!=0:\n",
    "                index_VFE_lchange=index_v_follow_ego_at_lchange[0]        \n",
    "            else:\n",
    "                index_VFE_lchange=len(all_index)+1\n",
    "                print(\"zero\")#,index_VPE_lchange)        \n",
    "        else:\n",
    "            index_VFE_lchange=len(all_index)+1\n",
    "            print(\"zero zero\")\n",
    "\n",
    "        data_index_required_VFE=[]\n",
    "        for i in range(len(data_index_required_E)):\n",
    "            if index_VFE_lchange==len(all_index)+1:\n",
    "                data_index_required_VFE.append(index_VFE_lchange)    \n",
    "            else:\n",
    "                data_index_required_VFE.append(index_VFE_lchange-i-1)\n",
    "\n",
    "        temp_extra_time=[]\n",
    "        \n",
    "        for i in range(extra_time_steps):\n",
    "            if index_VFE_lchange==len(all_index)+1:\n",
    "                temp_extra_time.append(index_VFE_lchange)    \n",
    "            else:\n",
    "                temp_extra_time.append(index_VFE_lchange+i)\n",
    "\n",
    "        data_index_required_VFE.extend(temp_extra_time)\n",
    "        data_index_required_VFE.sort()\n",
    "        ############################################################\n",
    "        \n",
    "        # Adding extra time steps after merging - for EGO vehicle\n",
    "        temp_extra_time_ego=[]\n",
    "        for i in range(extra_time_steps):\n",
    "            temp_extra_time_ego.append(data_index_required_E[len(data_index_required_E)-1]+i)\n",
    "        data_index_required_E.extend(temp_extra_time_ego)\n",
    "        \n",
    "        key=str(int(v_ego))\n",
    "        vehicle_row_indices[key]=[data_index_required_VPE,data_index_required_E,data_index_required_VFE]\n",
    "\n",
    "        # Velocity - v_ego#\n",
    "\n",
    "        for row_index in data_index_required_E[1:]:\n",
    "                vel_X=0\n",
    "                vel_Y=0                \n",
    "                dt=0.1 # Sec\n",
    "                                \n",
    "                vel_X=(df1.iat[row_index,4]-df1.iat[row_index-1,4])/dt\n",
    "                df1.iat[row_index,18]=round(vel_X,3)\n",
    "                \n",
    "                vel_y=(df1.iat[row_index,5]-df1.iat[row_index-1,5])/dt\n",
    "                df1.iat[row_index,19]=round(vel_y,3)  \n",
    "    \n",
    "        # Velocity - v_preced_ego VPE#\n",
    "\n",
    "        for row_index in data_index_required_VPE[1:]:\n",
    "                vel_X=0\n",
    "                vel_Y=0                \n",
    "                dt=0.1 # Sec\n",
    "                                \n",
    "                vel_X=(df1.iat[row_index,4]-df1.iat[row_index-1,4])/dt\n",
    "                df1.iat[row_index,18]=round(vel_X,3)\n",
    "                \n",
    "                vel_y=(df1.iat[row_index,5]-df1.iat[row_index-1,5])/dt\n",
    "                df1.iat[row_index,19]=round(vel_y,3)  \n",
    "        \n",
    "        # Velocity - v_follow_ego VFE#\n",
    "\n",
    "        for row_index in data_index_required_VFE[1:]:\n",
    "                vel_X=0\n",
    "                vel_Y=0                \n",
    "                dt=0.1 # Sec\n",
    "                                \n",
    "                vel_X=(df1.iat[row_index,4]-df1.iat[row_index-1,4])/dt\n",
    "                df1.iat[row_index,18]=round(vel_X,3)\n",
    "                \n",
    "                vel_y=(df1.iat[row_index,5]-df1.iat[row_index-1,5])/dt\n",
    "                df1.iat[row_index,19]=round(vel_y,3)  \n",
    "    \n",
    "\n",
    "# Train test split  \n",
    "vehicles_for_training=[]\n",
    "vehicles_for_testing=[]\n",
    "\n",
    "index=0\n",
    "for vehicle in vehicles_actually_merging:\n",
    "    if vehicle not in vehicles_for_training:\n",
    "        if index%10==0:\n",
    "            vehicles_for_testing.append(vehicle)\n",
    "            index+=1\n",
    "        else:\n",
    "            vehicles_for_training.append(vehicle)\n",
    "            index+=1\n",
    "\n",
    "\n",
    "dict_taining_data={}\n",
    "for vehicle in vehicles_for_training:\n",
    "    key=str(int(vehicle))\n",
    "    data_index=vehicle_row_indices[key]\n",
    "    ref_trajectory=[]\n",
    "\n",
    "    for index_VPE,index_E,index_VFE in zip(data_index[0][1:],data_index[1][1:],data_index[2][1:]):\n",
    "        temp_traj_point=[df1.iat[index_VPE,4],df1.iat[index_VPE,5],df1.iat[index_VPE,18],df1.iat[index_VPE,19]]\n",
    "        temp_traj_point.extend([df1.iat[index_E,4],df1.iat[index_E,5],df1.iat[index_E,18],df1.iat[index_E,19]])\n",
    "        temp_traj_point.extend([df1.iat[index_VFE,4],df1.iat[index_VFE,5],df1.iat[index_VFE,18],df1.iat[index_VFE,19]])\n",
    "        ref_trajectory.append(temp_traj_point)\n",
    "    dict_taining_data[key]=ref_trajectory    \n",
    "\n",
    "dict_test_data={}\n",
    "for vehicle in vehicles_for_testing:\n",
    "    key=str(int(vehicle))\n",
    "    data_index=vehicle_row_indices[key]\n",
    "    ref_trajectory=[]\n",
    "\n",
    "    for index_VPE,index_E,index_VFE in zip(data_index[0][1:],data_index[1][1:],data_index[2][1:]):\n",
    "        temp_traj_point=[df1.iat[index_VPE,4],df1.iat[index_VPE,5],df1.iat[index_VPE,18],df1.iat[index_VPE,19]]\n",
    "        temp_traj_point.extend([df1.iat[index_E,4],df1.iat[index_E,5],df1.iat[index_E,18],df1.iat[index_E,19]])\n",
    "        temp_traj_point.extend([df1.iat[index_VFE,4],df1.iat[index_VFE,5],df1.iat[index_VFE,18],df1.iat[index_VFE,19]])\n",
    "        ref_trajectory.append(temp_traj_point)\n",
    "    dict_test_data[key]=ref_trajectory   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################\n",
    "################################ AIS_gen ################################\n",
    "\n",
    "################################ Model 1 ################################ \n",
    "\n",
    "class GenerateAis1(nn.Module):\n",
    "\n",
    "    #def __init__(self,n_input,n_state,n_psi2_in=64,n_psi2_out=128):\n",
    "    def __init__(self,n_input,n_state,n_psi2_in=8,n_psi2_out=16):\n",
    "        super(GenerateAis1,self).__init__()\n",
    "        self.PSI_layer1=nn.Linear(n_input,n_psi2_in)     # Use RELU after\n",
    "        self.PSI_layer2=nn.Linear(n_psi2_in,n_psi2_out)      # Use RELU after\n",
    "        self.PSI_layer3=nn.GRUCell(n_psi2_out,n_state)       # This is the Gated layer\n",
    "\n",
    "    def forward(self,x,h):\n",
    "        #x=torch.transpose(x,0,1)\n",
    "        #h=torch.transpose(h,0,1)\n",
    "        x=torch.relu(self.PSI_layer1(x))\n",
    "        x=torch.relu(self.PSI_layer2(x))\n",
    "        h=self.PSI_layer3(x,h)\n",
    "        return h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "################################ AIS_pred ###############################\n",
    "\n",
    "################################ Model 1 ################################ \n",
    "\n",
    "class PredictAis1(nn.Module):\n",
    "    \n",
    "    #def __init__(self,n_output,n_state,n_phi2_in=16,n_phi2_out=8):\n",
    "    def __init__(self,n_output,n_state,n_phi2_in=32,n_phi2_out=64):\n",
    "        super(PredictAis1,self).__init__()\n",
    "        self.PHI_layer1=nn.Linear(n_state,n_phi2_in)  # Use RELU after\n",
    "        self.PHI_layer2=nn.Linear(n_phi2_in,n_phi2_out)     # Use RELU after\n",
    "        self.PHI_layer3=nn.Linear(n_phi2_out,n_output)         # mean vector of a unit-variance multivariate Gaussian distribution, samples from which are used to predict the next observation\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=torch.relu(self.PHI_layer1(x))\n",
    "        x=torch.relu(self.PHI_layer2(x))\n",
    "        output=self.PHI_layer3(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_structure(object):\n",
    "    def __init__(self,n_train,n_test,n_input,n_output,n_state,learning_rate,ais_gen_model,ais_pred_model,device):\n",
    "        self.n_train=n_train\n",
    "        self.n_test=n_test\n",
    "        self.n_input=n_input\n",
    "        self.n_output=n_output\n",
    "        self.n_state=n_state    # Hidden state size in RNN\n",
    "        self.learning_rate=learning_rate                \n",
    "        self.device=device\n",
    "        self.ais_gen_model=ais_gen_model\n",
    "        self.ais_pred_model=ais_pred_model\n",
    "        self.n_psi2_in=8\n",
    "        self.n_psi2_out=16\n",
    "        self.n_phi2_in=32\n",
    "        self.n_phi2_out=64\n",
    "        \n",
    "        if ais_gen_model==1:\n",
    "            self.gen_model=GenerateAis1(self.n_input,self.n_state,self.n_psi2_in,self.n_psi2_out).to(self.device)      \n",
    "        \n",
    "        if ais_pred_model==1:\n",
    "            self.pred_model=PredictAis1(self.n_output,self.n_state,self.n_phi2_in,self.n_phi2_out).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "    def save_model_weights(self,text=\"_\"):\n",
    "        name_gen_path=\"trained_models/ais_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_epochs\"+str(self.n_train)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state)+text+\".pth\"\n",
    "        torch.save(self.gen_model.state_dict(),name_gen_path)\n",
    "\n",
    "        name_pred_path=\"trained_models/ais_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_epochs\"+str(self.n_train)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state)+text+\".pth\"\n",
    "        torch.save(self.pred_model.state_dict(),name_pred_path)\n",
    "\n",
    "        return \"saved\"\n",
    "    \n",
    "    def load_model_weights(self,text=\"_\"):\n",
    "        name_gen_path=\"trained_models/ais_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_pred\"+str(self.ais_pred_model)+\"_epochs\"+str(self.n_train)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state)+text+\".pth\"\n",
    "        #torch.save(self.gen_model.state_dict(),name_gen_path)\n",
    "        self.gen_model.load_state_dict(torch.load(name_gen_path))\n",
    "        self.gen_model.eval()\n",
    "\n",
    "\n",
    "        name_pred_path=\"trained_models/ais_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_epochs\"+str(self.n_train)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state)+text+\".pth\"\n",
    "        #torch.save(self.pred_model.state_dict(),name_pred_path)\n",
    "        self.pred_model.load_state_dict(torch.load(name_pred_path))\n",
    "        self.pred_model.eval()\n",
    "\n",
    "        return \"loaded\"\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        train_bar=pyprind.ProgBar(self.n_train)\n",
    "        self.optimizer = torch.optim.Adam(list(self.gen_model.parameters()) + list(self.pred_model.parameters()), lr=self.learning_rate, amsgrad=True)\n",
    "        #loss=nn.MSELoss()\n",
    "        #torch.distributions.MultivariateNormal(obs_pred_next_probs[0,:], torch.eye(obs_pred_next_probs[0,:].shape[0]).to(device)).log_prob(obs[step+1,:])\n",
    "\n",
    "        for epochs in range(self.n_train):\n",
    "            train_bar.update()\n",
    "            #print(\"epoch\")\n",
    "            training_car_list=list(dict_taining_data.keys())\n",
    "            training_car_list=sample(training_car_list,len(training_car_list))# Random order for training\n",
    "\n",
    "            \n",
    "            for car in training_car_list:\n",
    "                trajectory_data=dict_taining_data[str(int(car))]\n",
    "\n",
    "                # Ais initialization\n",
    "                ais=0.5*torch.ones(self.n_state,dtype=torch.float).to(self.device)\n",
    "\n",
    "                car_loss=0                \n",
    "                    \n",
    "                for time in range(len(trajectory_data)-1): \n",
    "                    origin_offset_VPE=np.array(trajectory_data[0][0:2]) # VPE only for position\n",
    "                    origin_offset_E=np.array(trajectory_data[0][4:6]) # E only for position\n",
    "                    origin_offset_VFE=np.array(trajectory_data[0][8:10]) # VFE only for position\n",
    "                    \n",
    "                    input_vector=np.array(trajectory_data[time])\n",
    "                    input_vector[0:2]=input_vector[0:2]-origin_offset_VPE\n",
    "                    input_vector[4:6]=input_vector[4:6]-origin_offset_E\n",
    "                    input_vector[8:10]=input_vector[8:10]-origin_offset_VFE\n",
    "\n",
    "                    ref_vector=np.array(trajectory_data[time+1])\n",
    "                    ref_vector[0:2]=ref_vector[0:2]-origin_offset_VPE\n",
    "                    ref_vector[4:6]=ref_vector[4:6]-origin_offset_E\n",
    "                    ref_vector[8:10]=ref_vector[8:10]-origin_offset_VFE\n",
    "\n",
    "                    train_input=torch.tensor(input_vector,dtype=torch.float).to(self.device)\n",
    "                    # Output/Ref is only for the Ego vehicle\n",
    "                    ref_output=torch.tensor(ref_vector[4:8],dtype=torch.float).to(self.device)\n",
    "\n",
    "                    ais=self.gen_model(train_input,ais)\n",
    "                    pred_output=self.pred_model(ais)\n",
    "                    \n",
    "                    car_loss+=-torch.distributions.MultivariateNormal(pred_output,torch.eye(pred_output.shape[0]).to(device)).log_prob(ref_output)\n",
    "                    \n",
    "                self.optimizer.zero_grad()\n",
    "                car_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        print(\"saving model weights\")\n",
    "        status=self.save_model_weights()\n",
    "\n",
    "        return status\n",
    "\n",
    "\n",
    "    def test(self,vehicle):\n",
    "\n",
    "        # Testing for only one car data\n",
    "        # We test predictions based only newest observation\n",
    "        # Later we will test it only using initial condition\n",
    "\n",
    "        dict_test_results={}\n",
    "\n",
    "        for epochs in range(self.n_test):\n",
    "            test_car=vehicle            \n",
    "        \n",
    "            trajectory_data=dict_test_data[str(int(test_car))]\n",
    "\n",
    "            # Ais initialization\n",
    "            ais=0.5*torch.ones(self.n_state,dtype=torch.float).to(self.device)\n",
    "\n",
    "            car_loss=0\n",
    "            previous_prediction=0\n",
    "\n",
    "            test_results_trajectory=[]\n",
    "            for time in range(len(trajectory_data)-1): \n",
    "\n",
    "                temp_test_info=[]\n",
    "\n",
    "                origin_offset_VPE=np.array(trajectory_data[0][0:2]) # VPE only for position\n",
    "                origin_offset_E=np.array(trajectory_data[0][4:6]) # E only for position\n",
    "                origin_offset_VFE=np.array(trajectory_data[0][8:10]) # VFE only for position\n",
    "                \n",
    "                input_vector=np.array(trajectory_data[time])\n",
    "                input_vector[0:2]=input_vector[0:2]-origin_offset_VPE\n",
    "                input_vector[4:6]=input_vector[4:6]-origin_offset_E\n",
    "                input_vector[8:10]=input_vector[8:10]-origin_offset_VFE\n",
    "\n",
    "                ref_vector=np.array(trajectory_data[time+1])\n",
    "                ref_vector[0:2]=ref_vector[0:2]-origin_offset_VPE\n",
    "                ref_vector[4:6]=ref_vector[4:6]-origin_offset_E\n",
    "                ref_vector[8:10]=ref_vector[8:10]-origin_offset_VFE\n",
    "\n",
    "                # Output/Ref is only for the Ego vehicle\n",
    "                temp_test_info.extend(list(ref_vector[4:8]))\n",
    "\n",
    "                train_input=torch.tensor(input_vector,dtype=torch.float).to(self.device)\n",
    "                ref_output=torch.tensor(ref_vector[4:8],dtype=torch.float).to(self.device)\n",
    "\n",
    "                ais=self.gen_model(train_input,ais)\n",
    "                pred_output=self.pred_model(ais)\n",
    "\n",
    "                pred_vector=pred_output.detach().cpu().numpy()\n",
    "\n",
    "                temp_test_info.extend(list(pred_vector))\n",
    "\n",
    "                test_results_trajectory.append(temp_test_info)\n",
    "            \n",
    "            dict_test_results[str(int(test_car))]=test_results_trajectory\n",
    "\n",
    "\n",
    "        return dict_test_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Parameters #######\n",
    "#n_train=len(sampled_index_all_init)\n",
    "n_train=1\n",
    "n_test=1\n",
    "n_input=12\n",
    "n_output=4\n",
    "n_state =24 \n",
    "learning_rate=0.0003\n",
    "gen_model=1\n",
    "pred_model=1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'saved'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### Initialise the neural network #######\n",
    "\n",
    "network=NN_structure(n_train,n_test,n_input,n_output,n_state,learning_rate,gen_model,pred_model,device)\n",
    "print(\"Training has started\")\n",
    "network.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loaded'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### Load the neural network weights from saved models #######\n",
    "\n",
    "loaded_network=NN_structure(n_train,n_test,n_input,n_output,n_state,learning_rate,gen_model,pred_model,device)\n",
    "loaded_network.load_model_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_car=vehicles_for_testing[13]\n",
    "predicted_outputs=loaded_network.test(test_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting outputs #\n",
    "\n",
    "\n",
    "x_ref=[]\n",
    "y_ref=[]\n",
    "vx_ref=[]\n",
    "vy_ref=[]\n",
    "x_pred=[]\n",
    "y_pred=[]\n",
    "vx_pred=[]\n",
    "vy_pred=[]\n",
    "\n",
    "for elem in predicted_outputs[str(int(test_car))]:\n",
    "    x_ref.append(-elem[0])\n",
    "    y_ref.append(elem[1])\n",
    "    vx_ref.append(elem[2])\n",
    "    vy_ref.append(elem[3])\n",
    "    x_pred.append(-elem[4])\n",
    "    y_pred.append(elem[5])\n",
    "    vx_pred.append(elem[6])\n",
    "    vy_pred.append(elem[7])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#plt.plot(y_ref,x_ref,'r',linewidth=2.0,label=\"Actual\")\n",
    "#plt.plot(y_pred,x_pred,'b',linewidth=2.0,label=\"Predicted\")\n",
    "plt.plot(y_ref,'r',linewidth=2.0,label=\"Actual\")\n",
    "plt.plot(y_pred,'b',linewidth=2.0,label=\"Predicted\")\n",
    "plt.savefig('graphs/Comparision_xref_xpred_justonecar_g1_p1_lr0003_g_8_16_p_16_32.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git add .\n",
    "git branch -M main\n",
    "git commit -m \"adding all files\"\n",
    "git push -u origin main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19aadfa955e96a5e7fdf9b64cb086b0ad6aa114b0d422a86116b9caa90456979"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
