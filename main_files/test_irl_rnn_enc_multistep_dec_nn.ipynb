{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy.linalg as LA\n",
    "import random\n",
    "import pandas as pd\n",
    "from numpy import inf\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "import pyprind\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('data_irl_model/Feb23_data_exploratory_policy.json')\n",
    "f = open('../data_irl_model/Feb22_dataSVO.json')\n",
    "data_irl = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys=list(data_irl.keys())\n",
    "data_keys.sort()\n",
    "\n",
    "keys_for_training=[]\n",
    "keys_for_testing=[]\n",
    "\n",
    "index=0\n",
    "for vehicle in data_keys:\n",
    "    if index%10==0:\n",
    "        keys_for_testing.append(vehicle)\n",
    "        index+=1\n",
    "    else:\n",
    "        keys_for_training.append(vehicle)\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keys_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_irl[str(keys_for_testing[10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  [-50.0, -48.1, -46.4, -44.75796913937893, -43.040772632427355, -41.271099841265446, -39.4745449925036, -37.67293664052113, -35.88498649497113, -34.127188049008055, -32.415130406928334, -30.764126999474023, -29.19012274020666, -27.71120188485676, -26.348038955235424, -25.125573218590144, -24.074698426038893, -23.218097074828766, -22.561495723618638, -22.10489437240851, -21.848293021198383, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.76999234559332, -21.73049042434515, -21.59148658184881, -21.332482739352468, -20.95347889685613, -20.45447505435979, -19.83547121186345, -19.09646736936711, -18.237463526870773, -17.258459684374436, -16.1594558418781, -14.940451999381757, -13.601448156885418, -12.142444314389081, -10.56344047189274, -8.864436629396401, -7.045432786900065, -5.106428944403724, -3.0507800165161, -0.8879787967249797]\n",
      "1:  [-64.12587032688057, -62.17080987326437, -60.09574941964816, -57.939762556526794, -55.75793882767633, -53.572134785936, -51.39047619170394, -49.216295908249236, -47.051260899091936, -44.8964621324303, -42.752789574201906, -40.621048907221194, -38.501980742479894, -36.396230364340596, -34.30427419363369, -32.22630450635055, -30.16204687672467, -28.11048881916987, -26.06971526853422, -24.0368618037634, -22.00798095631169, -19.977883197437492, -17.940456611614778, -15.890189073109312, -13.82202072655911, -11.730069787325235, -9.60769269339216, -7.447645280146816, -5.242390979617483, -2.9845901341843444, -0.667771601616721]\n",
      "num_timesteps:  20\n"
     ]
    }
   ],
   "source": [
    "key=keys_for_training[967]\n",
    "key=str(key)                \n",
    "############# Create the data for the training locally- we are restricting the number of timesteps to num_timesteps######\n",
    "negative_vals=[]\n",
    "for i in [0,1]:\n",
    "    negative_vals.append(list(filter(lambda x: x < 1, data_irl[key][i])))\n",
    "num_timesteps=min(20,len(negative_vals[0]),len(negative_vals[1]))\n",
    "\n",
    "print(\"0: \", negative_vals[0])\n",
    "print(\"1: \", negative_vals[1])\n",
    "\n",
    "print(\"num_timesteps: \", num_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_before_zero=data_irl[key][1].index(negative_vals[1][len(negative_vals[1])-1])\n",
    "print(\"index_before_zero: \", index_before_zero)\n",
    "num_timesteps=20\n",
    "indices_to_train=[]\n",
    "if index_before_zero-num_timesteps>=0:\n",
    "    indices_to_train=range(index_before_zero-num_timesteps,index_before_zero+2)\n",
    "else:\n",
    "    indices_to_train=range(0,index_before_zero+2)\n",
    "\n",
    "for ind in indices_to_train:\n",
    "    print(data_irl[key][0][ind])\n",
    "for ind in indices_to_train:\n",
    "    print(data_irl[key][1][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "################################ AIS_gen ################################\n",
    "\n",
    "################################ Model 1 ################################ \n",
    "\n",
    "class GenerateAis1(nn.Module):\n",
    "\n",
    "    #def __init__(self,n_input,n_state,n_psi2_in=64,n_psi2_out=128):\n",
    "    def __init__(self,n_input,n_state,n_psi2_in=8,n_psi2_out=16):\n",
    "        super(GenerateAis1,self).__init__()\n",
    "        self.PSI_layer1=nn.Linear(n_input,n_psi2_in)     # Use RELU after\n",
    "        self.PSI_layer2=nn.Linear(n_psi2_in,n_psi2_out)      # Use RELU after\n",
    "        self.PSI_layer3=nn.GRUCell(n_psi2_out,n_state)       # This is the Gated layer\n",
    "\n",
    "    def forward(self,x,h):\n",
    "        #x=torch.transpose(x,0,1)\n",
    "        #h=torch.transpose(h,0,1)\n",
    "        x=torch.relu(self.PSI_layer1(x))\n",
    "        x=torch.relu(self.PSI_layer2(x))\n",
    "        h=self.PSI_layer3(x,h)\n",
    "        return h     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "################################ AIS_pred ###############################\n",
    "\n",
    "################################ Model 1 ################################ \n",
    "\n",
    "class PredictAis1(nn.Module):\n",
    "    \n",
    "    #def __init__(self,n_output,n_state,n_phi2_in=16,n_phi2_out=8):\n",
    "    def __init__(self,n_output,n_state,n_phi2_in=6,n_phi2_out=8):\n",
    "        super(PredictAis1,self).__init__()\n",
    "        self.PHI_layer1=nn.Linear(n_state,n_phi2_in)  # Use RELU after\n",
    "        self.PHI_layer2=nn.Linear(n_phi2_in,n_phi2_out)     # Use RELU after\n",
    "        self.PHI_layer3=nn.Linear(n_phi2_out,n_output)         # mean vector of a unit-variance multivariate Gaussian distribution, samples from which are used to predict the next observation\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=torch.relu(self.PHI_layer1(x))\n",
    "        x=torch.relu(self.PHI_layer2(x))\n",
    "        output=self.PHI_layer3(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-50.0, -48.1, -46.4, -44.761122658207675, -43.05127992142175, -41.28274312577878, -39.46536936838444, -37.60696288521823, -35.71356531742613, -33.78968838023041, -31.838501427503736, -29.861986078108643, -27.861070285758846, -25.835754674634355, -23.785244071125202, -21.708096136013104, -19.60239595060883, -17.4659597386202, -15.29656284320157, -13.092178062307937, -10.851202938627893, -8.573166165623306, -6.259465148147179, -3.913311880225578, -1.5399957277141993, 0.8524037569850123], [-56.30088862147648, -54.726865665496135, -53.206261143616395, -51.740175800649745, -50.32985055901144, -48.97686996364336, -47.68307154610725, -46.450560206681246, -45.281725232740015, -44.17925946401058, -43.1461798219091, -42.18584795159752, -41.30198901475314, -40.49870563862401, -39.7804825945506, -39.15217590591451, -38.61897783980735, -38.186346917018525, -37.85989037368617, -37.64518666049085, -37.54753932832985, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.528616713070534, -37.52847857140968, -37.526677973697346, -37.52056957470596, -37.507750665229345, -37.48603942735342, -37.45345511262157, -37.40820003775357, -37.34864327112781, -37.27330587434462, -37.18084756246843, -37.070054650938914, -36.93982916454346, -36.78917899283422, -36.617208986013, -36.42311289499089, -36.20616606868286, -35.965718830401165, -35.70119046335473, -35.41206374269798, -35.09787995830398, -34.758234378488154, -34.392772110324316, -34.00118431702228, -33.583204757130815, -33.13860661414189, -32.66719958845401, -32.168827226648816, -31.643364465691754, -31.09071537202079, -30.51081105757413, -29.90360775665892, -29.269085049205852, -28.60724421741451, -27.918106724092404, -27.201712802146275, -26.458120145714233, -25.687402694346204, -24.889649502461165, -24.064963687043868, -23.213461447201375, -22.335271149789364, -21.430532475847613, -20.499395623059932, -19.542020559882232, -18.558576327368456, -17.549240385072338, -16.51419799771783, -15.453641659615371, -14.367770554059128, -13.256790045173659, -12.12091119989065, -10.960350337928745, -9.775328607824584, -8.566071587222552, -7.332808905775735, -6.075773889143036, -4.795203222688038, -3.4913366335954095, -2.164416590221542, -0.8146880175880042, 0.5576019719891505]]\n"
     ]
    }
   ],
   "source": [
    "negative_vals=[]\n",
    "for i in [0,1]:\n",
    "    negative_vals.append(list(filter(lambda x: x < 1, data_irl[str(keys_for_testing[0])][i])))\n",
    "print(negative_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_structure(object):\n",
    "    def __init__(self,n_epochs,min_timesteps,n_rollout,n_skips_per_rollout,n_test,n_input,n_output,n_state_enc,learning_rate,ais_gen_model,ais_pred_model,device):\n",
    "        self.n_epochs=n_epochs\n",
    "        self.min_timesteps=min_timesteps\n",
    "        self.rollout=n_rollout\n",
    "        self.n_skips=n_skips_per_rollout\n",
    "        self.n_test=n_test\n",
    "        self.n_input=n_input\n",
    "        self.n_output=n_output\n",
    "        self.n_state_enc=n_state_enc    # Hidden state size in RNN\n",
    "        self.learning_rate=learning_rate                \n",
    "        self.device=device\n",
    "        self.ais_gen_model=ais_gen_model\n",
    "        self.ais_pred_model=ais_pred_model\n",
    "\n",
    "        self.n_psi2_in=8\n",
    "        self.n_psi2_out=16\n",
    "        self.n_phi2_in=2\n",
    "        self.n_phi2_out=4\n",
    "        self.n_psi1_out=8\n",
    "        \n",
    "        if ais_gen_model==1:\n",
    "            self.gen_model=GenerateAis1(self.n_input,self.n_state_enc).to(self.device)      \n",
    "        \n",
    "        #if ais_gen_model==2: \n",
    "            #self.gen_model=GenerateAis2(self.n_input,self.n_state_enc,self.n_psi1_out).to(self.device)        \n",
    "            #self.gen_model=GenerateAis2_LSTM(self.n_input,self.n_state_enc,self.n_psi1_out).to(self.device)\n",
    "            #self.gen_model=GenerateAis_LSTMShallow(self.n_input,self.n_state_enc).to(self.device)  \n",
    "\n",
    "        if ais_pred_model==1:\n",
    "            self.pred_model=PredictAis1(self.n_output,self.n_state_enc).to(self.device)\n",
    "        \n",
    "        #if ais_pred_model==2:\n",
    "            #self.pred_model=PredictAis2(self.n_output,self.n_state_enc+1,self.n_state_dec).to(self.device)\n",
    "            #self.pred_model=PredictAis2_LSTM(self.n_output,self.n_state_enc+1,self.n_state_dec).to(self.device)\n",
    "            #self.pred_model=PredictAis_LSTMShallow(self.n_state_enc+1,self.n_state_dec).to(self.device)\n",
    "\n",
    "    def save_model_weights(self,text=\"(:*_*:)\"):\n",
    "        #name_gen_path=\"trained_models/IRL_based_models/IRL_SVOpolicy_ais_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+text+\".pth\"\n",
    "        #torch.save(self.gen_model.state_dict(),name_gen_path)\n",
    "        \n",
    "        name_gen_path=\"_\"\n",
    "        name_pred_path=\"_\"\n",
    "        if self.ais_pred_model==1:\n",
    "            name_pred_path=\"../trained_models/IRL_based_models/IRL_SVOpolicy_ais_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+text+\".pth\"\n",
    "            name_gen_path=\"../trained_models/IRL_based_models/IRL_SVOpolicy_ais_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+text+\".pth\"\n",
    "\n",
    "        if self.ais_pred_model==2:\n",
    "            name_pred_path=\"../trained_models/IRL_based_models/IRL_SVOpolicy_multi_rnn_ais_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+\"_\"+str(self.n_state_dec)+text+\".pth\"\n",
    "            name_gen_path=\"../trained_models/IRL_based_models/IRL_SVOpolicy_multi_rnn_ais_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+\"_\"+str(self.n_state_dec)+text+\".pth\"\n",
    "\n",
    "        torch.save(self.gen_model.state_dict(),name_gen_path)\n",
    "        torch.save(self.pred_model.state_dict(),name_pred_path)\n",
    "\n",
    "        return \"saved\"\n",
    "    \n",
    "    def load_model_weights(self,text=\"(:*_*:)\"):\n",
    "        #name_gen_path=\"trained_models/IRL_based_models/IRL_SVOpolicy_ais_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+text+\".pth\"\n",
    "        #torch.save(self.gen_model.state_dict(),name_gen_path)\n",
    "        #self.gen_model.load_state_dict(torch.load(name_gen_path))\n",
    "        #self.gen_model.eval()\n",
    "\n",
    "        name_gen_path=\"_\"\n",
    "        name_pred_path=\"_\"\n",
    "        \n",
    "        if self.ais_pred_model==1:\n",
    "            name_pred_path=\"../trained_models/IRL_based_models/IRL_SVOpolicy_ais_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+text+\".pth\"\n",
    "            name_gen_path=\"../trained_models/IRL_based_models/IRL_SVOpolicy_ais_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+text+\".pth\"\n",
    "\n",
    "        if self.ais_pred_model==2:\n",
    "            name_pred_path=\"../trained_models/IRL_based_models/IRL_SVOpolicy_multi_rnn_ais_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+\"_\"+str(self.n_state_dec)+text+\".pth\"\n",
    "            name_gen_path=\"../trained_models/IRL_based_models/IRL_SVOpolicy_multi_rnn_ais_gen\"+str(self.ais_gen_model)+\"_\"+str(self.n_psi2_in)+str(self.n_psi2_out)+\"_pred\"+str(self.ais_pred_model)+\"_\"+str(self.n_phi2_in)+str(self.n_phi2_out)+\"_epochs\"+str(self.n_epochs)+\"_learning_rate\"+str(self.learning_rate)+\"_hidden_states\"+str(self.n_state_enc)+\"_\"+str(self.n_state_dec)+text+\".pth\"\n",
    "        \n",
    "        #torch.save(self.pred_model.state_dict(),name_pred_path)\n",
    "        self.gen_model.load_state_dict(torch.load(name_gen_path))\n",
    "        self.gen_model.eval()\n",
    "\n",
    "        self.pred_model.load_state_dict(torch.load(name_pred_path))\n",
    "        self.pred_model.eval()\n",
    "\n",
    "        return \"loaded\"\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        train_bar=pyprind.ProgBar(self.n_epochs)\n",
    "        self.optimizer = torch.optim.Adam(list(self.gen_model.parameters()) + list(self.pred_model.parameters()), lr=self.learning_rate, amsgrad=True)\n",
    "\n",
    "        time_step_loss=0\n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            train_bar.update()\n",
    "            keys_training=sample(keys_for_training,len(keys_for_training)) #shuffle the keys\n",
    "            #train_bar=pyprind.ProgBar(len(keys_for_training))\n",
    "            epoch_loss=0\n",
    "            for key in keys_training:\n",
    "                #for key in keys_for_testing[0]:\n",
    "                #train_bar.update()\n",
    "                key=str(key) \n",
    "                #print(key)                               \n",
    "                ############# Create the data for the training locally- we are restricting the number of timesteps to num_timesteps ######\n",
    "                negative_vals=[]\n",
    "                for i in [0,1]:\n",
    "                    negative_vals.append(list(filter(lambda x: x < 1, data_irl[key][i])))\n",
    "                num_timesteps=min(self.min_timesteps,len(negative_vals[0]),len(negative_vals[1]))\n",
    "                index_before_zero=data_irl[key][1].index(negative_vals[1][len(negative_vals[1])-1])\n",
    "                #print(\"index_before_zero: \", index_before_zero)\n",
    "                #num_timesteps=20\n",
    "                indices_to_train=[]\n",
    "                if index_before_zero-num_timesteps>=0:\n",
    "                    indices_to_train=range(index_before_zero-num_timesteps,index_before_zero+1)\n",
    "                else:\n",
    "                    indices_to_train=range(0,index_before_zero+2)\n",
    "                #print(\"ndices_to_train: \", indices_to_train)\n",
    "                #######################################\n",
    "\n",
    "                #### IMPORTANT Cropping the indices as per availability of data - \n",
    "                # What if index_before_zero == length of the list and we are looking \n",
    "                # to predict data that is in existent\n",
    "\n",
    "                '''\n",
    "                ## No skip between predictions\n",
    "                indices_to_train_cropped=[]\n",
    "                if index_before_zero in [len(data_irl[key][1])-1,len(data_irl[key][1])-2,len(data_irl[key][1])-3]:\n",
    "                    indices_to_train_cropped=indices_to_train[0:-1*self.rollout]\n",
    "                else:\n",
    "                    indices_to_train_cropped=indices_to_train\n",
    "                '''\n",
    "                \n",
    "                ## Use this when we want to skip between predictions\n",
    "                ## Skip indicates the predicitons are for t+n_skips,t+2*n_skips,t+3*n_skips,...\n",
    "                indices_to_train_cropped=[]\n",
    "                if index_before_zero in range(len(data_irl[key][1])-1,len(data_irl[key][1])-self.rollout*(self.n_skips+1),-1):\n",
    "                    indices_to_train_cropped=indices_to_train[0:-self.rollout*(self.n_skips+1)]\n",
    "                else:\n",
    "                    indices_to_train_cropped=indices_to_train\n",
    "\n",
    "                # Ais initialization\n",
    "                ais_rollout_enc=0.5*torch.ones(self.n_state_enc,dtype=torch.float).to(self.device)\n",
    "                state_rollout_enc=0.5*torch.ones(self.n_state_enc,dtype=torch.float).to(self.device) # Only if we use a LSTM\n",
    "                action_list=[0,0] ## initialize the action list differently - but we can try giving it the actual values to iterate                \n",
    "\n",
    "                time_step_loss=0\n",
    "\n",
    "                # Initialize the origin offset only for position\n",
    "                #origin_offset_CAV=data_irl[key][0][indices_to_train[0]]\n",
    "                #origin_offset_HDV=data_irl[key][1][indices_to_train[0]]\n",
    "                origin_offset_CAV=0    \n",
    "                origin_offset_HDV=0\n",
    "\n",
    "                for time_step in indices_to_train_cropped:\n",
    "\n",
    "                    observation_list=[]\n",
    "                    for i in range(len(data_irl[key])-2): # exclude the last two columns (actions/inputs)\n",
    "                        temp=data_irl[key][i][time_step]\n",
    "                        if i==0:\n",
    "                            temp=temp-origin_offset_CAV\n",
    "                        elif i==1:\n",
    "                            temp=temp-origin_offset_HDV\n",
    "                        if temp!= None:\n",
    "                            observation_list.append(temp)\n",
    "                    input_list=[]\n",
    "                    input_list.extend(observation_list)\n",
    "                    input_list.extend(action_list)\n",
    "                    input_tensor=torch.tensor(input_list,dtype=torch.float).to(self.device)                    \n",
    "\n",
    "                    # Update action list for the next time step \n",
    "                    for i in [4,5]:\n",
    "                        temp=data_irl[key][i][time_step]\n",
    "                        if temp!= None:\n",
    "                            action_list[i-4]=temp\n",
    "                        else:\n",
    "                            action_list[i-4]=0\n",
    "\n",
    "                    '''\n",
    "                    ### For no skip between predictions\n",
    "                    reference_list=[]\n",
    "                    for k in range(self.rollout):\n",
    "                        for i in [1,3,5]:#range(len(data_irl[key])):\n",
    "                            if i==1:# min is self.rollout\n",
    "                                #rint(\"key\",key)\n",
    "                                #print(\"i\",i)\n",
    "                                #print(\"time_step+k+1\",time_step+k+1)\n",
    "                                reference_list.append(data_irl[key][i][time_step+k+1]-origin_offset_HDV)\n",
    "                            \n",
    "                            elif i==3:\n",
    "                                reference_list.append(data_irl[key][i][time_step+k+1])\n",
    "\n",
    "                            elif i>=4 and data_irl[key][i][time_step+k]!=None: # Previous action is observed at time_step+k+1\n",
    "                                reference_list.append(data_irl[key][i][time_step+k])\n",
    "                            \n",
    "                            else:\n",
    "                                reference_list.append(0) # Zero padding for the input action\n",
    "\n",
    "                    '''\n",
    "\n",
    "                    \n",
    "                    ### Skip between predictions\n",
    "                    reference_list=[]\n",
    "                    if (time_step+self.rollout*(self.n_skips+1))>=len(data_irl[key][i]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        for k in range(self.n_skips,self.rollout*(self.n_skips+1),self.n_skips+1):\n",
    "                            for i in [1,3,5]:#range(len(data_irl[key])):\n",
    "                                if i==1:# min is self.rollout\n",
    "                                    #rint(\"key\",key)\n",
    "                                    #print(\"i\",i)\n",
    "                                    #print(\"time_step+k+1\",time_step+k+1)\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k+1]-origin_offset_HDV)\n",
    "                                \n",
    "                                elif i==3:\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k+1])\n",
    "\n",
    "                                elif i>=4 and data_irl[key][i][time_step+k]!=None: # Previous action is observed at time_step+k+1\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k])\n",
    "                                \n",
    "                                else:\n",
    "                                    reference_list.append(0) # Zero padding for the input action\n",
    "\n",
    "                                           \n",
    "\n",
    "                    '''\n",
    "                    # This is to be as 3* as long as the number of rollouts - 3 for each rollout\n",
    "                    internal_rollout=min(self.rollout,len(data_irl[key][0])-time_step-2)\n",
    "                    \n",
    "\n",
    "                    reference_list=[]\n",
    "\n",
    "                    if internal_rollout==self.rollout:\n",
    "                        for k in range(self.rollout):\n",
    "                            for i in [1,3,5]:#range(len(data_irl[key])):\n",
    "                                if i==1:# min is self.rollout\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k+1]-origin_offset_HDV)\n",
    "                                \n",
    "                                elif i==3:\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k+1])\n",
    "\n",
    "                                elif i>=4 and data_irl[key][i][time_step+k]!=None: # Previous action is observed at time_step+k+1\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k])\n",
    "                                \n",
    "                                else:\n",
    "                                    reference_list.append(0) # Zero padding for the input action\n",
    "                            \n",
    "                    else:\n",
    "                        for k in range(internal_rollout):\n",
    "                            for i in [1,3,5]:#range(len(data_irl[key])):\n",
    "                                if i==1:# min is not self.rollout\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k+1]-origin_offset_HDV)\n",
    "                                \n",
    "                                elif i==3:\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k+1])\n",
    "\n",
    "                                elif i>=4 and data_irl[key][i][time_step+k]!=None: # Previous action is observed at time_step+k+1\n",
    "                                    reference_list.append(data_irl[key][i][time_step+k])\n",
    "                                \n",
    "                                else:\n",
    "                                    reference_list.append(0) # Zero padding for the input action\n",
    "                                    \n",
    "                        for k in range(internal_rollout,self.rollout):\n",
    "                            for i in [1,3,5]:\n",
    "                                reference_list.append(0)\n",
    "                    '''\n",
    "\n",
    "\n",
    "                    reference_tensor=torch.tensor(reference_list,dtype=torch.float).to(self.device)\n",
    "\n",
    "                    ais_rollout_enc=self.gen_model(input_tensor,ais_rollout_enc)\n",
    "                    decoder_prediction=self.pred_model(ais_rollout_enc)\n",
    "\n",
    "                    ## Mulitvariate normal distribution loss\n",
    "                    #time_step_loss+=torch.distributions.MultivariateNormal(decoder_prediction,torch.eye(self.n_output).to(self.device)).log_prob(reference_tensor)\n",
    "                    ## MSE loss\n",
    "                    time_step_loss+=torch.nn.functional.mse_loss(decoder_prediction,reference_tensor)                    \n",
    "                    #print(\"time_step_loss: \",time_step_loss)\n",
    "                    #print(\"reference_tensor: \",reference_tensor)\n",
    "                    #print(\"decoder_prediction: \",decoder_prediction,\"\\n\")\n",
    "                    \n",
    "                    \n",
    "               \n",
    "                epoch_loss+=time_step_loss\n",
    "                self.optimizer.zero_grad()\n",
    "                time_step_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Printing the gradients\n",
    "                '''\n",
    "                print(\"Printing the gradients-encoder\")\n",
    "                print(self.gen_model.PSI_layer3.weight_ih.grad)\n",
    "                print(self.gen_model.PSI_layer3.weight_hh.grad)\n",
    "                print(self.gen_model.PSI_layer3.bias_ih.grad)\n",
    "                print(self.gen_model.PSI_layer3.bias_hh.grad)\n",
    "\n",
    "                print(\"Printing the gradients-decoder\")                \n",
    "                print(self.pred_model.PHI_layer1.weight.grad)\n",
    "                print(self.pred_model.PHI_layer2.weight.grad)\n",
    "                print(self.pred_model.PHI_layer3.weight.grad)           \n",
    "                '''\n",
    "                \n",
    "            print(\"epoch_loss is \",epoch_loss,\"\\n\")                \n",
    "            print(\"saving model weights for epoch \",epoch)\n",
    "            status=self.save_model_weights(\"encoder_rnn_decoder_simple_nn_n_rollout_3\")\n",
    "\n",
    "    def test(self,test_key):\n",
    "        test_results=[]\n",
    "        time_step_loss=0\n",
    "        keys_testing=keys_for_testing[test_key]# sample(keys_for_training,len(keys_for_training)) #shuffle the keys\n",
    "        #train_bar=pyprind.ProgBar(len(keys_for_training))\n",
    "        epoch_loss=0\n",
    "        for key in keys_testing:\n",
    "            #for key in keys_for_testing[0]:\n",
    "            #train_bar.update()\n",
    "            key=str(key) \n",
    "            #print(key)                               \n",
    "            ############# Create the data for the training locally- we are restricting the number of timesteps to num_timesteps ######\n",
    "            negative_vals=[]\n",
    "            for i in [0,1]:\n",
    "                negative_vals.append(list(filter(lambda x: x < 1, data_irl[key][i])))\n",
    "            num_timesteps=min(self.min_timesteps,len(negative_vals[0]),len(negative_vals[1]))\n",
    "            index_before_zero=data_irl[key][1].index(negative_vals[1][len(negative_vals[1])-1])\n",
    "            #print(\"index_before_zero: \", index_before_zero)\n",
    "            #num_timesteps=20\n",
    "            indices_to_test=[]\n",
    "            if index_before_zero-num_timesteps>=0:\n",
    "                indices_to_test=range(index_before_zero-num_timesteps,index_before_zero+1)\n",
    "            else:\n",
    "                indices_to_test=range(0,index_before_zero+2)\n",
    "            #print(\"ndices_to_train: \", indices_to_train)\n",
    "            #######################################\n",
    "\n",
    "            #### IMPORTANT Cropping the indices as per availability of data - \n",
    "            # What if index_before_zero == length of the list and we are looking \n",
    "            # to predict data that is in existent\n",
    "\n",
    "            '''\n",
    "            ## No skip between predictions\n",
    "            indices_to_train_cropped=[]\n",
    "            if index_before_zero in [len(data_irl[key][1])-1,len(data_irl[key][1])-2,len(data_irl[key][1])-3]:\n",
    "                indices_to_train_cropped=indices_to_train[0:-1*self.rollout]\n",
    "            else:\n",
    "                indices_to_train_cropped=indices_to_train\n",
    "            '''\n",
    "            \n",
    "            ## Use this when we want to skip between predictions\n",
    "            ## Skip indicates the predicitons are for t+n_skips,t+2*n_skips,t+3*n_skips,...\n",
    "            indices_to_test_cropped=[]\n",
    "            if index_before_zero in range(len(data_irl[key][1])-1,len(data_irl[key][1])-self.rollout*(self.n_skips+1),-1):\n",
    "                indices_to_test_cropped=indices_to_test[0:-self.rollout*(self.n_skips+1)]\n",
    "            else:\n",
    "                indices_to_test_cropped=indices_to_test\n",
    "\n",
    "            # Ais initialization\n",
    "            ais_rollout_enc=0.5*torch.ones(self.n_state_enc,dtype=torch.float).to(self.device)\n",
    "            state_rollout_enc=0.5*torch.ones(self.n_state_enc,dtype=torch.float).to(self.device) # Only if we use a LSTM\n",
    "            action_list=[0,0] ## initialize the action list differently - but we can try giving it the actual values to iterate                \n",
    "\n",
    "            time_step_loss=0\n",
    "\n",
    "            # Initialize the origin offset only for position\n",
    "            #origin_offset_CAV=data_irl[key][0][indices_to_train[0]]\n",
    "            #origin_offset_HDV=data_irl[key][1][indices_to_train[0]]\n",
    "            \n",
    "            # For now we are not doing origin offset \n",
    "            origin_offset_CAV=0    \n",
    "            origin_offset_HDV=0\n",
    "\n",
    "            for time_step in indices_to_test_cropped:\n",
    "\n",
    "                observation_list=[]\n",
    "                for i in range(len(data_irl[key])-2): # exclude the last two columns (actions/inputs)\n",
    "                    temp=data_irl[key][i][time_step]\n",
    "                    if i==0:\n",
    "                        temp=temp-origin_offset_CAV\n",
    "                    elif i==1:\n",
    "                        temp=temp-origin_offset_HDV\n",
    "                    if temp!= None:\n",
    "                        observation_list.append(temp)\n",
    "                input_list=[]\n",
    "                input_list.extend(observation_list)\n",
    "                input_list.extend(action_list)\n",
    "                input_tensor=torch.tensor(input_list,dtype=torch.float).to(self.device)                    \n",
    "\n",
    "                # Update action list for the next time step \n",
    "                for i in [4,5]:\n",
    "                    temp=data_irl[key][i][time_step]\n",
    "                    if temp!= None:\n",
    "                        action_list[i-4]=temp\n",
    "                    else:\n",
    "                        action_list[i-4]=0\n",
    "\n",
    "                '''\n",
    "                ### For no skip between predictions\n",
    "                reference_list=[]\n",
    "                for k in range(self.rollout):\n",
    "                    for i in [1,3,5]:#range(len(data_irl[key])):\n",
    "                        if i==1:# min is self.rollout\n",
    "                            #rint(\"key\",key)\n",
    "                            #print(\"i\",i)\n",
    "                            #print(\"time_step+k+1\",time_step+k+1)\n",
    "                            reference_list.append(data_irl[key][i][time_step+k+1]-origin_offset_HDV)\n",
    "                        \n",
    "                        elif i==3:\n",
    "                            reference_list.append(data_irl[key][i][time_step+k+1])\n",
    "\n",
    "                        elif i>=4 and data_irl[key][i][time_step+k]!=None: # Previous action is observed at time_step+k+1\n",
    "                            reference_list.append(data_irl[key][i][time_step+k])\n",
    "                        \n",
    "                        else:\n",
    "                            reference_list.append(0) # Zero padding for the input action\n",
    "\n",
    "                '''\n",
    "\n",
    "                \n",
    "                ### Skip between predictions\n",
    "                reference_list=[]\n",
    "                if (time_step+self.rollout*(self.n_skips+1))>=len(data_irl[key][i]):\n",
    "                    continue\n",
    "                else:\n",
    "                    for k in range(self.n_skips,self.rollout*(self.n_skips+1),self.n_skips+1):\n",
    "                        #print(range(self.n_skips,self.rollout*(self.n_skips+1),self.n_skips+1))\n",
    "                        #print(\"k\",k,\"\\n\")\n",
    "                        for i in [1,3,5]:#range(len(data_irl[key])):\n",
    "                            if i==1:# min is self.rollout\n",
    "                                #rint(\"key\",key)\n",
    "                                #print(\"i\",i)\n",
    "                                #print(\"time_step+k+1\",time_step+k+1)\n",
    "                                reference_list.append(data_irl[key][i][time_step+k+1]-origin_offset_HDV)\n",
    "                            \n",
    "                            elif i==3:\n",
    "                                reference_list.append(data_irl[key][i][time_step+k+1])\n",
    "\n",
    "                            elif i>=4 and data_irl[key][i][time_step+k]!=None: # Previous action is observed at time_step+k+1\n",
    "                                reference_list.append(data_irl[key][i][time_step+k])\n",
    "                            \n",
    "                            else:\n",
    "                                reference_list.append(0) # Zero padding for the input action\n",
    "\n",
    "\n",
    "                reference_tensor=torch.tensor(reference_list,dtype=torch.float).to(self.device)\n",
    "                \n",
    "                temp=[]\n",
    "                temp.append(reference_tensor.detach().cpu().numpy())\n",
    "\n",
    "                ais_rollout_enc=self.gen_model(input_tensor,ais_rollout_enc)\n",
    "                decoder_prediction=self.pred_model(ais_rollout_enc)\n",
    "\n",
    "                ## Mulitvariate normal distribution loss\n",
    "                #time_step_loss+=torch.distributions.MultivariateNormal(decoder_prediction,torch.eye(self.n_output).to(self.device)).log_prob(reference_tensor)\n",
    "                ## MSE loss\n",
    "                time_step_loss+=torch.nn.functional.mse_loss(decoder_prediction,reference_tensor)                    \n",
    "                #print(\"time_step_loss: \",time_step_loss)\n",
    "                print(\"time_step: \",time_step)\n",
    "                print(\"reference_tensor: \",reference_tensor)\n",
    "                print(\"decoder_prediction: \",decoder_prediction,\"\\n\")\n",
    "\n",
    "                temp.append(decoder_prediction.detach().cpu().numpy())\n",
    "                test_results.append(temp)\n",
    "            \n",
    "        return test_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Parameters #######\n",
    "n_epochs=150\n",
    "min_timesteps=30\n",
    "n_rollout=3\n",
    "n_skips_per_rollout=1\n",
    "n_test=1\n",
    "n_input=6\n",
    "n_output=n_rollout*3 # 3 for each of the next n_rollout time steps\n",
    "n_state_enc=4\n",
    "learning_rate=0.0003\n",
    "gen_model=1\n",
    "pred_model=1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n",
      "epoch_loss is  tensor(6794951.5000, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  0\n",
      "epoch_loss is  tensor(305949.0938, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  1\n",
      "epoch_loss is  tensor(168722.3594, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  2\n",
      "epoch_loss is  tensor(119809.2656, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#                             ] 100% | ETA: 08:42:18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(94028.3125, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  4\n",
      "epoch_loss is  tensor(85959.7188, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  5\n",
      "epoch_loss is  tensor(70665.7109, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  6\n",
      "epoch_loss is  tensor(55459.9844, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##                            ] 100% | ETA: 09:05:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(51536.4883, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  8\n",
      "epoch_loss is  tensor(50609.4648, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  9\n",
      "epoch_loss is  tensor(56143.7148, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  10\n",
      "epoch_loss is  tensor(57706.1172, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  11\n",
      "epoch_loss is  tensor(42726.9023, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [###                           ] 100% | ETA: 09:09:13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(40821.5117, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  13\n",
      "epoch_loss is  tensor(41728.1406, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  14\n",
      "epoch_loss is  tensor(41496.9102, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  15\n",
      "epoch_loss is  tensor(38769.5117, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  16\n",
      "epoch_loss is  tensor(36568.9375, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  17\n",
      "epoch_loss is  tensor(42100.0234, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [####                          ] 100% | ETA: 08:54:30"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(34789.0234, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  19\n",
      "epoch_loss is  tensor(34285.5312, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  20\n",
      "epoch_loss is  tensor(34771.5664, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  21\n",
      "epoch_loss is  tensor(37237.4336, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#####                         ] 100% | ETA: 08:40:57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(34129.8320, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  23\n",
      "epoch_loss is  tensor(32989.1875, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  24\n",
      "epoch_loss is  tensor(31470.4570, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  25\n",
      "epoch_loss is  tensor(31847.0742, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  26\n",
      "epoch_loss is  tensor(30506.8652, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [######                        ] 100% | ETA: 08:26:37"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(32884.7227, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  28\n",
      "epoch_loss is  tensor(30816.6309, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  29\n",
      "epoch_loss is  tensor(30032.9395, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  30\n",
      "epoch_loss is  tensor(32878.4688, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  31\n",
      "epoch_loss is  tensor(29181.9199, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  32\n",
      "epoch_loss is  tensor(28463.3027, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#######                       ] 100% | ETA: 08:02:25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(29951.6504, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  34\n",
      "epoch_loss is  tensor(30151.5898, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  35\n",
      "epoch_loss is  tensor(28349.5352, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  36\n",
      "epoch_loss is  tensor(29793.0625, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [########                      ] 100% | ETA: 07:46:42"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(27443.9258, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  38\n",
      "epoch_loss is  tensor(29818.5234, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  39\n",
      "epoch_loss is  tensor(27898.5801, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  40\n",
      "epoch_loss is  tensor(26797.1816, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  41\n",
      "epoch_loss is  tensor(26456.2812, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#########                     ] 100% | ETA: 07:28:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(26258.0059, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  43\n",
      "epoch_loss is  tensor(26710.5059, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  44\n",
      "epoch_loss is  tensor(26033.0820, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  45\n",
      "epoch_loss is  tensor(26429.1406, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  46\n",
      "epoch_loss is  tensor(25690.9941, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  47\n",
      "epoch_loss is  tensor(25582.7383, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##########                    ] 100% | ETA: 07:03:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(25939.7988, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  49\n",
      "epoch_loss is  tensor(25070.5352, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  50\n",
      "epoch_loss is  tensor(24855.5879, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  51\n",
      "epoch_loss is  tensor(24394.0703, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [###########                   ] 100% | ETA: 06:47:44"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(24650.6094, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  53\n",
      "epoch_loss is  tensor(24404.5625, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  54\n",
      "epoch_loss is  tensor(24908.9570, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  55\n",
      "epoch_loss is  tensor(25531.8789, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  56\n",
      "epoch_loss is  tensor(24182.6426, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [############                  ] 100% | ETA: 06:27:11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(23664.6953, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  58\n",
      "epoch_loss is  tensor(24711.2695, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  59\n",
      "epoch_loss is  tensor(23850.7480, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  60\n",
      "epoch_loss is  tensor(23193.3730, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  61\n",
      "epoch_loss is  tensor(23330.3027, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  62\n",
      "epoch_loss is  tensor(22721.5605, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#############                 ] 100% | ETA: 06:01:43"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(23004.4766, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  64\n",
      "epoch_loss is  tensor(22453.4590, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  65\n",
      "epoch_loss is  tensor(22710.7930, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  66\n",
      "epoch_loss is  tensor(22120.8359, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############                ] 100% | ETA: 05:45:10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(22014.7090, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  68\n",
      "epoch_loss is  tensor(22050.9395, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  69\n",
      "epoch_loss is  tensor(21370.4102, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  70\n",
      "epoch_loss is  tensor(22621.5605, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  71\n",
      "epoch_loss is  tensor(21219.1641, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [###############               ] 100% | ETA: 05:24:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(21308.7266, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  73\n",
      "epoch_loss is  tensor(20888.0078, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  74\n",
      "epoch_loss is  tensor(21138.8711, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  75\n",
      "epoch_loss is  tensor(20850.2188, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  76\n",
      "epoch_loss is  tensor(20680.9902, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  77\n",
      "epoch_loss is  tensor(20532.5547, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [################              ] 100% | ETA: 04:58:14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(20952.5566, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  79\n",
      "epoch_loss is  tensor(21173.5664, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  80\n",
      "epoch_loss is  tensor(20200.5371, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  81\n",
      "epoch_loss is  tensor(20392.5078, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#################             ] 100% | ETA: 04:41:38"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(20492.9336, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  83\n",
      "epoch_loss is  tensor(19587.7051, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  84\n",
      "epoch_loss is  tensor(20083.8691, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  85\n",
      "epoch_loss is  tensor(19712.0449, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  86\n",
      "epoch_loss is  tensor(19589.9180, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##################            ] 100% | ETA: 04:19:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(19387.4883, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  88\n",
      "epoch_loss is  tensor(19045.5449, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  89\n",
      "epoch_loss is  tensor(18931.5234, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  90\n",
      "epoch_loss is  tensor(18794.9297, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  91\n",
      "epoch_loss is  tensor(19094.3965, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  92\n",
      "epoch_loss is  tensor(18444.7246, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [###################           ] 100% | ETA: 03:54:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(18872.2715, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  94\n",
      "epoch_loss is  tensor(18147.6523, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  95\n",
      "epoch_loss is  tensor(18293.4883, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  96\n",
      "epoch_loss is  tensor(18359.4160, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [####################          ] 100% | ETA: 03:36:57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(17623.4160, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  98\n",
      "epoch_loss is  tensor(17348.1152, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  99\n",
      "epoch_loss is  tensor(17246.8926, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  100\n",
      "epoch_loss is  tensor(16768.3438, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  101\n",
      "epoch_loss is  tensor(16546.9375, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#####################         ] 100% | ETA: 03:15:15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(16731.5605, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  103\n",
      "epoch_loss is  tensor(16531.8262, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  104\n",
      "epoch_loss is  tensor(16262.8594, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  105\n",
      "epoch_loss is  tensor(16145.4766, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  106\n",
      "epoch_loss is  tensor(16038.8740, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  107\n",
      "epoch_loss is  tensor(15743.8887, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [######################        ] 100% | ETA: 02:49:29"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(15884.1992, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  109\n",
      "epoch_loss is  tensor(15915.7686, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  110\n",
      "epoch_loss is  tensor(15261.4482, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  111\n",
      "epoch_loss is  tensor(15315.8330, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#######################       ] 100% | ETA: 02:32:11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(14869.1777, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  113\n",
      "epoch_loss is  tensor(15024.5352, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  114\n",
      "epoch_loss is  tensor(14906.7705, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  115\n",
      "epoch_loss is  tensor(14884.4580, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  116\n",
      "epoch_loss is  tensor(14799.7822, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [########################      ] 100% | ETA: 02:10:25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(14575.4756, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  118\n",
      "epoch_loss is  tensor(14599.7607, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  119\n",
      "epoch_loss is  tensor(14421.5420, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  120\n",
      "epoch_loss is  tensor(14689.9551, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  121\n",
      "epoch_loss is  tensor(14405.1025, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  122\n",
      "epoch_loss is  tensor(14364.0010, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#########################     ] 100% | ETA: 01:44:32"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(14054.1162, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  124\n",
      "epoch_loss is  tensor(13998.2578, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  125\n",
      "epoch_loss is  tensor(14784.4316, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  126\n",
      "epoch_loss is  tensor(14215.5615, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##########################    ] 100% | ETA: 01:27:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(13797.8408, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  128\n",
      "epoch_loss is  tensor(13891.1191, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  129\n",
      "epoch_loss is  tensor(13838.9043, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  130\n",
      "epoch_loss is  tensor(13848.3115, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  131\n",
      "epoch_loss is  tensor(13819.1562, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [###########################   ] 100% | ETA: 01:05:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(13719.9502, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  133\n",
      "epoch_loss is  tensor(13772.5938, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  134\n",
      "epoch_loss is  tensor(13447.7627, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  135\n",
      "epoch_loss is  tensor(13459.8203, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  136\n",
      "epoch_loss is  tensor(15809.1934, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  137\n",
      "epoch_loss is  tensor(13306.1260, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [############################  ] 100% | ETA: 00:39:15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(13542.4219, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  139\n",
      "epoch_loss is  tensor(13458.1602, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  140\n",
      "epoch_loss is  tensor(13313.4805, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  141\n",
      "epoch_loss is  tensor(13658.9141, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [############################# ] 100% | ETA: 00:21:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(13093.1816, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  143\n",
      "epoch_loss is  tensor(13028.3760, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  144\n",
      "epoch_loss is  tensor(13054.7070, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  145\n",
      "epoch_loss is  tensor(13091.4111, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  146\n",
      "epoch_loss is  tensor(13008.3379, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(13346.0127, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 10:54:15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss is  tensor(13094.1250, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "saving model weights for epoch  149\n"
     ]
    }
   ],
   "source": [
    "####### Initialise the neural network #######\n",
    "\n",
    "network=NN_structure(n_epochs,min_timesteps,n_rollout,n_skips_per_rollout,n_test,n_input,n_output,n_state_enc,learning_rate,gen_model,pred_model,device)\n",
    "print(\"Training has started\")\n",
    "network.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Sorting issues with indexing ##########################\n",
    "#key= 2825\n",
    "#i= 1\n",
    "#time_step+k+1 =201\n",
    "\n",
    "#data_irl[str(2825)][1][200]\n",
    "\n",
    "key=str(1289)\n",
    "\n",
    "############# Create the data for the training locally- we are restricting the number of timesteps to num_timesteps ######\n",
    "negative_vals=[]\n",
    "for i in [0,1]:\n",
    "    negative_vals.append(list(filter(lambda x: x < 1, data_irl[key][i])))\n",
    "num_timesteps=min(15,len(negative_vals[0]),len(negative_vals[1]))\n",
    "index_before_zero=data_irl[key][1].index(negative_vals[1][len(negative_vals[1])-1])\n",
    "#print(\"index_before_zero: \", index_before_zero)\n",
    "num_timesteps=20\n",
    "indices_to_train=[]\n",
    "if index_before_zero-num_timesteps>=0:\n",
    "    indices_to_train=range(index_before_zero-num_timesteps,index_before_zero+1)\n",
    "    print(\"ndices_to_train: if \", indices_to_train)\n",
    "else:\n",
    "    indices_to_train=range(0,index_before_zero+2)\n",
    "    print(\"ndices_to_train: else \", indices_to_train)\n",
    "#######################################\n",
    "print(\"values and index near zero\",index_before_zero,len(data_irl[key][1]))\n",
    "#print(data_irl[key][1][index_before_zero-1])\n",
    "#print(data_irl[key][1][index_before_zero])\n",
    "#print(data_irl[key][1][index_before_zero+1])\n",
    "\n",
    "#### IMPORTANT Cropping the indices as per availability of data - \n",
    "# What if index_before_zero == length of the list and we are looking \n",
    "# to predict data that is in existent\n",
    "\n",
    "n_rollout=4\n",
    "n_skip_per_rollout=2\n",
    "indices_to_train_cropped=[]\n",
    "if index_before_zero in range(len(data_irl[key][1])-1,len(data_irl[key][1])-n_rollout*(n_skip_per_rollout+1),-1):\n",
    "    #[len(data_irl[key][1])-1,len(data_irl[key][1])-2,len(data_irl[key][1])-3]\n",
    "    #indices_to_train_cropped=indices_to_train[0:-n_rollout]\n",
    "    indices_to_train_cropped=indices_to_train[0:-n_rollout*(n_skip_per_rollout+1)]\n",
    "else:\n",
    "    indices_to_train_cropped=indices_to_train\n",
    "print(\"indices_to_train_cropped\",indices_to_train_cropped)\n",
    "print(\"length of array\",len(data_irl[key][i]))\n",
    "print(\"min(n_rollout*(n_skip_per_rollout+1),len(data_irl[key][i]))\",min(n_rollout*(n_skip_per_rollout+1),len(data_irl[key][i])))\n",
    "\n",
    "for time_step in indices_to_train_cropped:\n",
    "    reference_list=[]\n",
    "    print(\"timestep_where skip began\",time_step)\n",
    "    if (time_step+n_rollout*(n_skip_per_rollout+1))>=len(data_irl[key][i]):\n",
    "        continue\n",
    "    else:\n",
    "        for k in range(n_skip_per_rollout,n_rollout*(n_skip_per_rollout+1),n_skip_per_rollout+1):\n",
    "            i=1# min is self.rollout\n",
    "            print(\"key\",key)\n",
    "            print(\"i\",i)\n",
    "            print(\"time_step+k+1\",time_step+k+1)\n",
    "            reference_list.append(data_irl[key][i][time_step+k+1])\n",
    "            print(reference_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(169, 178)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a12=range(169, 190)\n",
    "n_rollout=4\n",
    "n_skip_per_rollout=2\n",
    "a12[0:-n_rollout*(n_skip_per_rollout+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_step:  7\n",
      "reference_tensor:  tensor([-4.1345e+01,  7.6099e+00, -1.9274e-02, -3.8303e+01,  7.6018e+00,\n",
      "        -2.0753e-02, -3.5264e+01,  7.5930e+00, -2.2337e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-41.2701,   8.0067,   0.2849, -38.0412,   8.1030,   0.1716, -34.7890,\n",
      "          8.1742,   0.1147], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  8\n",
      "reference_tensor:  tensor([-3.9823e+01,  7.6059e+00, -1.9998e-02, -3.6783e+01,  7.5975e+00,\n",
      "        -2.1535e-02, -3.3746e+01,  7.5884e+00, -2.3151e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.9938e+01,  7.7452e+00,  1.3604e-01, -3.6811e+01,  7.7997e+00,\n",
      "         5.5785e-02, -3.3690e+01,  7.8410e+00,  2.5527e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  9\n",
      "reference_tensor:  tensor([-3.8303e+01,  7.6018e+00, -2.0753e-02, -3.5264e+01,  7.5930e+00,\n",
      "        -2.2337e-02, -3.2228e+01,  7.5836e+00, -2.3963e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.8245e+01,  7.5570e+00,  5.1411e-02, -3.5195e+01,  7.5885e+00,\n",
      "        -4.1467e-03, -3.2160e+01,  7.6146e+00, -1.5501e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  10\n",
      "reference_tensor:  tensor([-3.6783e+01,  7.5975e+00, -2.1535e-02, -3.3746e+01,  7.5884e+00,\n",
      "        -2.3151e-02, -3.0712e+01,  7.5786e+00, -2.4752e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.6678e+01,  7.5020e+00,  2.5614e-02, -3.3654e+01,  7.5272e+00,\n",
      "        -1.7434e-02, -3.0642e+01,  7.5502e+00, -1.9760e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  11\n",
      "reference_tensor:  tensor([-3.5264e+01,  7.5930e+00, -2.2337e-02, -3.2228e+01,  7.5836e+00,\n",
      "        -2.3963e-02, -2.9197e+01,  7.5735e+00, -2.5492e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.5155e+01,  7.4876e+00,  1.8405e-02, -3.2141e+01,  7.5117e+00,\n",
      "        -1.6060e-02, -2.9131e+01,  7.5356e+00, -1.2516e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  12\n",
      "reference_tensor:  tensor([-3.3746e+01,  7.5884e+00, -2.3151e-02, -3.0712e+01,  7.5786e+00,\n",
      "        -2.4752e-02, -2.7683e+01,  7.5683e+00, -2.6143e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.3642e+01,  7.4821e+00,  1.7329e-02, -3.0634e+01,  7.5070e+00,\n",
      "        -9.7851e-03, -2.7622e+01,  7.5331e+00, -1.3643e-03], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  13\n",
      "reference_tensor:  tensor([-3.2228e+01,  7.5836e+00, -2.3963e-02, -2.9197e+01,  7.5735e+00,\n",
      "        -2.5492e-02, -2.6170e+01,  7.5630e+00, -2.6659e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.2131e+01,  7.4782e+00,  1.9150e-02, -2.9128e+01,  7.5047e+00,\n",
      "        -1.1414e-03, -2.6114e+01,  7.5338e+00,  1.1723e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  14\n",
      "reference_tensor:  tensor([-3.0712e+01,  7.5786e+00, -2.4752e-02, -2.7683e+01,  7.5683e+00,\n",
      "        -2.6143e-02, -2.4658e+01,  7.5576e+00, -2.6977e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.0621e+01,  7.4742e+00,  2.2929e-02, -2.7623e+01,  7.5030e+00,\n",
      "         9.1277e-03, -2.4607e+01,  7.5355e+00,  2.6159e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  15\n",
      "reference_tensor:  tensor([-2.9197e+01,  7.5735e+00, -2.5492e-02, -2.6170e+01,  7.5630e+00,\n",
      "        -2.6659e-02, -2.3147e+01,  7.5522e+00, -2.7031e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-2.9114e+01,  7.4691e+00,  2.8695e-02, -2.6122e+01,  7.5008e+00,\n",
      "         2.1080e-02, -2.3102e+01,  7.5372e+00,  4.2019e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  16\n",
      "reference_tensor:  tensor([-2.7683e+01,  7.5683e+00, -2.6143e-02, -2.4658e+01,  7.5576e+00,\n",
      "        -2.6977e-02, -2.1637e+01,  7.5468e+00, -2.6752e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-27.6117,   7.4637,   0.0352, -24.6258,   7.4984,   0.0337, -21.6030,\n",
      "          7.5389,   0.0584], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  17\n",
      "reference_tensor:  tensor([-2.6170e+01,  7.5630e+00, -2.6659e-02, -2.3147e+01,  7.5522e+00,\n",
      "        -2.7031e-02, -2.0128e+01,  7.5416e+00, -2.6128e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-26.1142,   7.4558,   0.0429, -23.1354,   7.4940,   0.0473, -20.1102,\n",
      "          7.5388,   0.0757], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  18\n",
      "reference_tensor:  tensor([-24.6576,   7.5576,  -0.0270, -21.6367,   7.5468,  -0.0268, -18.6200,\n",
      "          7.5365,  -0.0253], device='cuda:0')\n",
      "decoder_prediction:  tensor([-24.6244,   7.4466,   0.0502, -21.6534,   7.4880,   0.0607, -18.6263,\n",
      "          7.5372,   0.0929], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  19\n",
      "reference_tensor:  tensor([-23.1466,   7.5522,  -0.0270, -20.1279,   7.5416,  -0.0261, -17.1132,\n",
      "          7.5317,  -0.0244], device='cuda:0')\n",
      "decoder_prediction:  tensor([-23.1509,   7.4431,   0.0667, -20.1861,   7.4905,   0.0818, -17.1528,\n",
      "          7.5460,   0.1165], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  20\n",
      "reference_tensor:  tensor([-21.6367,   7.5468,  -0.0268, -18.6200,   7.5365,  -0.0253, -15.6074,\n",
      "          7.5270,  -0.0232], device='cuda:0')\n",
      "decoder_prediction:  tensor([-21.6388,   7.4667,   0.0983, -18.6681,   7.5247,   0.1146, -15.6159,\n",
      "          7.5901,   0.1492], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  21\n",
      "reference_tensor:  tensor([-20.1279,   7.5416,  -0.0261, -17.1132,   7.5317,  -0.0244, -14.1024,\n",
      "          7.5226,  -0.0219], device='cuda:0')\n",
      "decoder_prediction:  tensor([-20.0927,   7.4882,   0.1420, -17.1151,   7.5607,   0.1571, -14.0437,\n",
      "          7.6394,   0.1896], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  22\n",
      "reference_tensor:  tensor([-18.6200,   7.5365,  -0.0253, -15.6074,   7.5270,  -0.0232, -12.5983,\n",
      "          7.5186,  -0.0203], device='cuda:0')\n",
      "decoder_prediction:  tensor([-18.5606,   7.4985,   0.1771, -15.5790,   7.5832,   0.1918, -12.4954,\n",
      "          7.6735,   0.2233], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  23\n",
      "reference_tensor:  tensor([-17.1132,   7.5317,  -0.0244, -14.1024,   7.5226,  -0.0219, -11.0949,\n",
      "          7.5149,  -0.0185], device='cuda:0')\n",
      "decoder_prediction:  tensor([-17.0813,   7.4938,   0.1422, -14.1000,   7.5694,   0.1677, -11.0246,\n",
      "          7.6543,   0.2076], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  24\n",
      "reference_tensor:  tensor([-15.6074,   7.5270,  -0.0232, -12.5983,   7.5186,  -0.0203,  -9.5923,\n",
      "          7.5116,  -0.0164], device='cuda:0')\n",
      "decoder_prediction:  tensor([-15.5604,   7.4652,   0.0860, -12.5894,   7.5252,   0.1267,  -9.5353,\n",
      "          7.6000,   0.1784], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  25\n",
      "reference_tensor:  tensor([-14.1024,   7.5226,  -0.0219, -11.0949,   7.5149,  -0.0185,  -8.0902,\n",
      "          7.5088,  -0.0142], device='cuda:0')\n",
      "decoder_prediction:  tensor([-14.0211,   7.4457,   0.0550, -11.0576,   7.4978,   0.1069,  -8.0159,\n",
      "          7.5684,   0.1670], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  26\n",
      "reference_tensor:  tensor([-1.2598e+01,  7.5186e+00, -2.0280e-02, -9.5923e+00,  7.5116e+00,\n",
      "        -1.6423e-02, -6.5887e+00,  7.5064e+00, -1.1688e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-12.4980,   7.4420,   0.0382,  -9.5358,   7.4905,   0.0986,  -6.4974,\n",
      "          7.5600,   0.1651], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  27\n",
      "reference_tensor:  tensor([-1.1095e+01,  7.5149e+00, -1.8466e-02, -8.0902e+00,  7.5088e+00,\n",
      "        -1.4159e-02, -5.0876e+00,  7.5046e+00, -9.0355e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-10.9843,   7.4428,   0.0266,  -8.0218,   7.4891,   0.0946,  -4.9840,\n",
      "          7.5588,   0.1665], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  28\n",
      "reference_tensor:  tensor([-9.5923e+00,  7.5116e+00, -1.6423e-02, -6.5887e+00,  7.5064e+00,\n",
      "        -1.1688e-02, -3.5868e+00,  7.5034e+00, -6.2315e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-9.5380,  7.4878,  0.0221, -6.5550,  7.5339,  0.0945, -3.5009,  7.6052,\n",
      "         0.1697], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  29\n",
      "reference_tensor:  tensor([-8.0902e+00,  7.5088e+00, -1.4159e-02, -5.0876e+00,  7.5046e+00,\n",
      "        -9.0355e-03, -2.0862e+00,  7.5027e+00, -3.3153e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-8.0909,  7.5225,  0.0175, -5.0900,  7.5685,  0.0940, -2.0248,  7.6418,\n",
      "         0.1722], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  30\n",
      "reference_tensor:  tensor([-6.5887e+00,  7.5064e+00, -1.1688e-02, -3.5868e+00,  7.5034e+00,\n",
      "        -6.2315e-03, -5.8566e-01,  7.5027e+00, -3.3159e-04], device='cuda:0')\n",
      "decoder_prediction:  tensor([-6.6174,  7.5310,  0.0128, -3.6086,  7.5773,  0.0936, -0.5426,  7.6530,\n",
      "         0.1753], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  31\n",
      "reference_tensor:  tensor([-5.0876e+00,  7.5046e+00, -9.0355e-03, -2.0862e+00,  7.5027e+00,\n",
      "        -3.3153e-03,  9.1493e-01,  7.5032e+00,  2.6714e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-5.1257,  7.5276,  0.0085, -2.1139,  7.5745,  0.0940,  0.9484,  7.6529,\n",
      "         0.1793], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  32\n",
      "reference_tensor:  tensor([-3.5868e+00,  7.5034e+00, -6.2315e-03, -5.8566e-01,  7.5027e+00,\n",
      "        -3.3159e-04,  2.4157e+00,  7.5043e+00,  5.6445e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.6197e+00,  7.5204e+00,  2.5076e-03, -6.0696e-01,  7.5673e+00,\n",
      "         9.3281e-02,  2.4502e+00,  7.6480e+00,  1.8245e-01], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  33\n",
      "reference_tensor:  tensor([-2.0862e+00,  7.5027e+00, -3.3153e-03,  9.1493e-01,  7.5032e+00,\n",
      "         2.6714e-03,  3.9167e+00,  7.5060e+00,  8.5400e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-2.1013,  7.5130, -0.0084,  0.9118,  7.5583,  0.0887,  3.9631,  7.6400,\n",
      "         0.1827], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  34\n",
      "reference_tensor:  tensor([-5.8566e-01,  7.5027e+00, -3.3159e-04,  2.4157e+00,  7.5043e+00,\n",
      "         5.6445e-03,  5.4181e+00,  7.5083e+00,  1.1314e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-0.5723,  7.5070, -0.0272,  2.4412,  7.5483,  0.0778,  5.4861,  7.6290,\n",
      "         0.1778], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  35\n",
      "reference_tensor:  tensor([9.1493e-01, 7.5032e+00, 2.6714e-03, 3.9167e+00, 7.5060e+00, 8.5400e-03,\n",
      "        6.9201e+00, 7.5111e+00, 1.3930e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 0.9655,  7.5037, -0.0570,  3.9798,  7.5375,  0.0579,  7.0177,  7.6143,\n",
      "         0.1656], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  36\n",
      "reference_tensor:  tensor([2.4157e+00, 7.5043e+00, 5.6445e-03, 5.4181e+00, 7.5083e+00, 1.1314e-02,\n",
      "        8.4226e+00, 7.5143e+00, 1.6357e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 2.5108,  7.5038, -0.1001,  5.5267,  7.5260,  0.0273,  8.5568,  7.5955,\n",
      "         0.1446], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  37\n",
      "reference_tensor:  tensor([3.9167e+00, 7.5060e+00, 8.5400e-03, 6.9201e+00, 7.5111e+00, 1.3930e-02,\n",
      "        9.9259e+00, 7.5181e+00, 1.8573e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 4.0632,  7.5083, -0.1580,  7.0819,  7.5141, -0.0155, 10.1033,  7.5726,\n",
      "         0.1136], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  7\n",
      "reference_tensor:  tensor([-4.1345e+01,  7.6099e+00, -1.9274e-02, -3.8303e+01,  7.6018e+00,\n",
      "        -2.0753e-02, -3.5264e+01,  7.5930e+00, -2.2337e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-41.2701,   8.0067,   0.2849, -38.0412,   8.1030,   0.1716, -34.7890,\n",
      "          8.1742,   0.1147], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  8\n",
      "reference_tensor:  tensor([-3.9823e+01,  7.6059e+00, -1.9998e-02, -3.6783e+01,  7.5975e+00,\n",
      "        -2.1535e-02, -3.3746e+01,  7.5884e+00, -2.3151e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.9938e+01,  7.7452e+00,  1.3604e-01, -3.6811e+01,  7.7997e+00,\n",
      "         5.5785e-02, -3.3690e+01,  7.8410e+00,  2.5527e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  9\n",
      "reference_tensor:  tensor([-3.8303e+01,  7.6018e+00, -2.0753e-02, -3.5264e+01,  7.5930e+00,\n",
      "        -2.2337e-02, -3.2228e+01,  7.5836e+00, -2.3963e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.8245e+01,  7.5570e+00,  5.1411e-02, -3.5195e+01,  7.5885e+00,\n",
      "        -4.1467e-03, -3.2160e+01,  7.6146e+00, -1.5501e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  10\n",
      "reference_tensor:  tensor([-3.6783e+01,  7.5975e+00, -2.1535e-02, -3.3746e+01,  7.5884e+00,\n",
      "        -2.3151e-02, -3.0712e+01,  7.5786e+00, -2.4752e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.6678e+01,  7.5020e+00,  2.5614e-02, -3.3654e+01,  7.5272e+00,\n",
      "        -1.7434e-02, -3.0642e+01,  7.5502e+00, -1.9760e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  11\n",
      "reference_tensor:  tensor([-3.5264e+01,  7.5930e+00, -2.2337e-02, -3.2228e+01,  7.5836e+00,\n",
      "        -2.3963e-02, -2.9197e+01,  7.5735e+00, -2.5492e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.5155e+01,  7.4876e+00,  1.8405e-02, -3.2141e+01,  7.5117e+00,\n",
      "        -1.6060e-02, -2.9131e+01,  7.5356e+00, -1.2516e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  12\n",
      "reference_tensor:  tensor([-3.3746e+01,  7.5884e+00, -2.3151e-02, -3.0712e+01,  7.5786e+00,\n",
      "        -2.4752e-02, -2.7683e+01,  7.5683e+00, -2.6143e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.3642e+01,  7.4821e+00,  1.7329e-02, -3.0634e+01,  7.5070e+00,\n",
      "        -9.7851e-03, -2.7622e+01,  7.5331e+00, -1.3643e-03], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  13\n",
      "reference_tensor:  tensor([-3.2228e+01,  7.5836e+00, -2.3963e-02, -2.9197e+01,  7.5735e+00,\n",
      "        -2.5492e-02, -2.6170e+01,  7.5630e+00, -2.6659e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.2131e+01,  7.4782e+00,  1.9150e-02, -2.9128e+01,  7.5047e+00,\n",
      "        -1.1414e-03, -2.6114e+01,  7.5338e+00,  1.1723e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  14\n",
      "reference_tensor:  tensor([-3.0712e+01,  7.5786e+00, -2.4752e-02, -2.7683e+01,  7.5683e+00,\n",
      "        -2.6143e-02, -2.4658e+01,  7.5576e+00, -2.6977e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.0621e+01,  7.4742e+00,  2.2929e-02, -2.7623e+01,  7.5030e+00,\n",
      "         9.1277e-03, -2.4607e+01,  7.5355e+00,  2.6159e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  15\n",
      "reference_tensor:  tensor([-2.9197e+01,  7.5735e+00, -2.5492e-02, -2.6170e+01,  7.5630e+00,\n",
      "        -2.6659e-02, -2.3147e+01,  7.5522e+00, -2.7031e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-2.9114e+01,  7.4691e+00,  2.8695e-02, -2.6122e+01,  7.5008e+00,\n",
      "         2.1080e-02, -2.3102e+01,  7.5372e+00,  4.2019e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  16\n",
      "reference_tensor:  tensor([-2.7683e+01,  7.5683e+00, -2.6143e-02, -2.4658e+01,  7.5576e+00,\n",
      "        -2.6977e-02, -2.1637e+01,  7.5468e+00, -2.6752e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-27.6117,   7.4637,   0.0352, -24.6258,   7.4984,   0.0337, -21.6030,\n",
      "          7.5389,   0.0584], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  17\n",
      "reference_tensor:  tensor([-2.6170e+01,  7.5630e+00, -2.6659e-02, -2.3147e+01,  7.5522e+00,\n",
      "        -2.7031e-02, -2.0128e+01,  7.5416e+00, -2.6128e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-26.1142,   7.4558,   0.0429, -23.1354,   7.4940,   0.0473, -20.1102,\n",
      "          7.5388,   0.0757], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  18\n",
      "reference_tensor:  tensor([-24.6576,   7.5576,  -0.0270, -21.6367,   7.5468,  -0.0268, -18.6200,\n",
      "          7.5365,  -0.0253], device='cuda:0')\n",
      "decoder_prediction:  tensor([-24.6244,   7.4466,   0.0502, -21.6534,   7.4880,   0.0607, -18.6263,\n",
      "          7.5372,   0.0929], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  19\n",
      "reference_tensor:  tensor([-23.1466,   7.5522,  -0.0270, -20.1279,   7.5416,  -0.0261, -17.1132,\n",
      "          7.5317,  -0.0244], device='cuda:0')\n",
      "decoder_prediction:  tensor([-23.1509,   7.4431,   0.0667, -20.1861,   7.4905,   0.0818, -17.1528,\n",
      "          7.5460,   0.1165], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  20\n",
      "reference_tensor:  tensor([-21.6367,   7.5468,  -0.0268, -18.6200,   7.5365,  -0.0253, -15.6074,\n",
      "          7.5270,  -0.0232], device='cuda:0')\n",
      "decoder_prediction:  tensor([-21.6388,   7.4667,   0.0983, -18.6681,   7.5247,   0.1146, -15.6159,\n",
      "          7.5901,   0.1492], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  21\n",
      "reference_tensor:  tensor([-20.1279,   7.5416,  -0.0261, -17.1132,   7.5317,  -0.0244, -14.1024,\n",
      "          7.5226,  -0.0219], device='cuda:0')\n",
      "decoder_prediction:  tensor([-20.0927,   7.4882,   0.1420, -17.1151,   7.5607,   0.1571, -14.0437,\n",
      "          7.6394,   0.1896], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  22\n",
      "reference_tensor:  tensor([-18.6200,   7.5365,  -0.0253, -15.6074,   7.5270,  -0.0232, -12.5983,\n",
      "          7.5186,  -0.0203], device='cuda:0')\n",
      "decoder_prediction:  tensor([-18.5606,   7.4985,   0.1771, -15.5790,   7.5832,   0.1918, -12.4954,\n",
      "          7.6735,   0.2233], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  23\n",
      "reference_tensor:  tensor([-17.1132,   7.5317,  -0.0244, -14.1024,   7.5226,  -0.0219, -11.0949,\n",
      "          7.5149,  -0.0185], device='cuda:0')\n",
      "decoder_prediction:  tensor([-17.0813,   7.4938,   0.1422, -14.1000,   7.5694,   0.1677, -11.0246,\n",
      "          7.6543,   0.2076], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  24\n",
      "reference_tensor:  tensor([-15.6074,   7.5270,  -0.0232, -12.5983,   7.5186,  -0.0203,  -9.5923,\n",
      "          7.5116,  -0.0164], device='cuda:0')\n",
      "decoder_prediction:  tensor([-15.5604,   7.4652,   0.0860, -12.5894,   7.5252,   0.1267,  -9.5353,\n",
      "          7.6000,   0.1784], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  25\n",
      "reference_tensor:  tensor([-14.1024,   7.5226,  -0.0219, -11.0949,   7.5149,  -0.0185,  -8.0902,\n",
      "          7.5088,  -0.0142], device='cuda:0')\n",
      "decoder_prediction:  tensor([-14.0211,   7.4457,   0.0550, -11.0576,   7.4978,   0.1069,  -8.0159,\n",
      "          7.5684,   0.1670], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  26\n",
      "reference_tensor:  tensor([-1.2598e+01,  7.5186e+00, -2.0280e-02, -9.5923e+00,  7.5116e+00,\n",
      "        -1.6423e-02, -6.5887e+00,  7.5064e+00, -1.1688e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-12.4980,   7.4420,   0.0382,  -9.5358,   7.4905,   0.0986,  -6.4974,\n",
      "          7.5600,   0.1651], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  27\n",
      "reference_tensor:  tensor([-1.1095e+01,  7.5149e+00, -1.8466e-02, -8.0902e+00,  7.5088e+00,\n",
      "        -1.4159e-02, -5.0876e+00,  7.5046e+00, -9.0355e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-10.9843,   7.4428,   0.0266,  -8.0218,   7.4891,   0.0946,  -4.9840,\n",
      "          7.5588,   0.1665], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  28\n",
      "reference_tensor:  tensor([-9.5923e+00,  7.5116e+00, -1.6423e-02, -6.5887e+00,  7.5064e+00,\n",
      "        -1.1688e-02, -3.5868e+00,  7.5034e+00, -6.2315e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-9.5380,  7.4878,  0.0221, -6.5550,  7.5339,  0.0945, -3.5009,  7.6052,\n",
      "         0.1697], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  29\n",
      "reference_tensor:  tensor([-8.0902e+00,  7.5088e+00, -1.4159e-02, -5.0876e+00,  7.5046e+00,\n",
      "        -9.0355e-03, -2.0862e+00,  7.5027e+00, -3.3153e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-8.0909,  7.5225,  0.0175, -5.0900,  7.5685,  0.0940, -2.0248,  7.6418,\n",
      "         0.1722], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  30\n",
      "reference_tensor:  tensor([-6.5887e+00,  7.5064e+00, -1.1688e-02, -3.5868e+00,  7.5034e+00,\n",
      "        -6.2315e-03, -5.8566e-01,  7.5027e+00, -3.3159e-04], device='cuda:0')\n",
      "decoder_prediction:  tensor([-6.6174,  7.5310,  0.0128, -3.6086,  7.5773,  0.0936, -0.5426,  7.6530,\n",
      "         0.1753], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  31\n",
      "reference_tensor:  tensor([-5.0876e+00,  7.5046e+00, -9.0355e-03, -2.0862e+00,  7.5027e+00,\n",
      "        -3.3153e-03,  9.1493e-01,  7.5032e+00,  2.6714e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-5.1257,  7.5276,  0.0085, -2.1139,  7.5745,  0.0940,  0.9484,  7.6529,\n",
      "         0.1793], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  32\n",
      "reference_tensor:  tensor([-3.5868e+00,  7.5034e+00, -6.2315e-03, -5.8566e-01,  7.5027e+00,\n",
      "        -3.3159e-04,  2.4157e+00,  7.5043e+00,  5.6445e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.6197e+00,  7.5204e+00,  2.5076e-03, -6.0696e-01,  7.5673e+00,\n",
      "         9.3281e-02,  2.4502e+00,  7.6480e+00,  1.8245e-01], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  33\n",
      "reference_tensor:  tensor([-2.0862e+00,  7.5027e+00, -3.3153e-03,  9.1493e-01,  7.5032e+00,\n",
      "         2.6714e-03,  3.9167e+00,  7.5060e+00,  8.5400e-03], device='cuda:0')\n",
      "decoder_prediction:  tensor([-2.1013,  7.5130, -0.0084,  0.9118,  7.5583,  0.0887,  3.9631,  7.6400,\n",
      "         0.1827], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  34\n",
      "reference_tensor:  tensor([-5.8566e-01,  7.5027e+00, -3.3159e-04,  2.4157e+00,  7.5043e+00,\n",
      "         5.6445e-03,  5.4181e+00,  7.5083e+00,  1.1314e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([-0.5723,  7.5070, -0.0272,  2.4412,  7.5483,  0.0778,  5.4861,  7.6290,\n",
      "         0.1778], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  35\n",
      "reference_tensor:  tensor([9.1493e-01, 7.5032e+00, 2.6714e-03, 3.9167e+00, 7.5060e+00, 8.5400e-03,\n",
      "        6.9201e+00, 7.5111e+00, 1.3930e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 0.9655,  7.5037, -0.0570,  3.9798,  7.5375,  0.0579,  7.0177,  7.6143,\n",
      "         0.1656], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  36\n",
      "reference_tensor:  tensor([2.4157e+00, 7.5043e+00, 5.6445e-03, 5.4181e+00, 7.5083e+00, 1.1314e-02,\n",
      "        8.4226e+00, 7.5143e+00, 1.6357e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 2.5108,  7.5038, -0.1001,  5.5267,  7.5260,  0.0273,  8.5568,  7.5955,\n",
      "         0.1446], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  37\n",
      "reference_tensor:  tensor([3.9167e+00, 7.5060e+00, 8.5400e-03, 6.9201e+00, 7.5111e+00, 1.3930e-02,\n",
      "        9.9259e+00, 7.5181e+00, 1.8573e-02], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 4.0632,  7.5083, -0.1580,  7.0819,  7.5141, -0.0155, 10.1033,  7.5726,\n",
      "         0.1136], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  23\n",
      "reference_tensor:  tensor([-30.2911,   6.0439,   0.0390, -27.8706,   6.0585,   0.0356, -25.4446,\n",
      "          6.0719,   0.0329], device='cuda:0')\n",
      "decoder_prediction:  tensor([-2.9500e+01,  5.4350e+00, -1.6457e-01, -2.7323e+01,  5.4413e+00,\n",
      "        -7.3507e-02, -2.5109e+01,  5.4764e+00,  8.8815e-03], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  24\n",
      "reference_tensor:  tensor([-29.0816,   6.0513,   0.0373, -26.6583,   6.0653,   0.0341, -24.2295,\n",
      "          6.0783,   0.0322], device='cuda:0')\n",
      "decoder_prediction:  tensor([-2.9170e+01,  6.1076e+00, -7.4050e-02, -2.6732e+01,  6.1191e+00,\n",
      "        -3.8591e-02, -2.4255e+01,  6.1475e+00,  1.4123e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  25\n",
      "reference_tensor:  tensor([-27.8706,   6.0585,   0.0356, -25.4446,   6.0719,   0.0329, -23.0132,\n",
      "          6.0847,   0.0321], device='cuda:0')\n",
      "decoder_prediction:  tensor([-27.9072,   6.1932,   0.0870, -25.4346,   6.2547,   0.0985, -22.8928,\n",
      "          6.3240,   0.1309], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  26\n",
      "reference_tensor:  tensor([-26.6583,   6.0653,   0.0341, -24.2295,   6.0783,   0.0322, -21.7956,\n",
      "          6.0913,   0.0327], device='cuda:0')\n",
      "decoder_prediction:  tensor([-26.5034,   6.1264,   0.1490, -24.0558,   6.2091,   0.1558, -21.5281,\n",
      "          6.2978,   0.1835], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  27\n",
      "reference_tensor:  tensor([-25.4446,   6.0719,   0.0329, -23.0132,   6.0847,   0.0321, -20.5767,\n",
      "          6.0981,   0.0341], device='cuda:0')\n",
      "decoder_prediction:  tensor([-25.2062,   6.0226,   0.1041, -22.7936,   6.0943,   0.1225, -20.3180,\n",
      "          6.1770,   0.1596], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  28\n",
      "reference_tensor:  tensor([-24.2295,   6.0783,   0.0322, -21.7956,   6.0913,   0.0327, -19.3564,\n",
      "          6.1053,   0.0361], device='cuda:0')\n",
      "decoder_prediction:  tensor([-23.9549,   6.0355,   0.0749, -21.5303,   6.0999,   0.1001, -19.0587,\n",
      "          6.1789,   0.1429], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  29\n",
      "reference_tensor:  tensor([-23.0132,   6.0847,   0.0321, -20.5767,   6.0981,   0.0341, -18.1345,\n",
      "          6.1130,   0.0386], device='cuda:0')\n",
      "decoder_prediction:  tensor([-22.8004,   6.1397,   0.0717, -20.3283,   6.2038,   0.0977, -17.8197,\n",
      "          6.2838,   0.1415], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  30\n",
      "reference_tensor:  tensor([-21.7956,   6.0913,   0.0327, -19.3564,   6.1053,   0.0361, -16.9111,\n",
      "          6.1213,   0.0413], device='cuda:0')\n",
      "decoder_prediction:  tensor([-21.6659,   6.2702,   0.0710, -19.1373,   6.3343,   0.0975, -16.5800,\n",
      "          6.4153,   0.1419], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  31\n",
      "reference_tensor:  tensor([-20.5767,   6.0981,   0.0341, -18.1345,   6.1130,   0.0386, -15.6860,\n",
      "          6.1301,   0.0440], device='cuda:0')\n",
      "decoder_prediction:  tensor([-20.6178,   6.2080,   0.0744, -18.1084,   6.2753,   0.1031, -15.5775,\n",
      "          6.3610,   0.1492], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  32\n",
      "reference_tensor:  tensor([-19.3564,   6.1053,   0.0361, -16.9111,   6.1213,   0.0413, -14.4590,\n",
      "          6.1394,   0.0466], device='cuda:0')\n",
      "decoder_prediction:  tensor([-19.3715,   6.2492,   0.1017, -16.8412,   6.3261,   0.1281, -14.2909,\n",
      "          6.4213,   0.1721], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  33\n",
      "reference_tensor:  tensor([-18.1345,   6.1130,   0.0386, -15.6860,   6.1301,   0.0440, -13.2302,\n",
      "          6.1492,   0.0489], device='cuda:0')\n",
      "decoder_prediction:  tensor([-18.1293,   5.9645,   0.0583, -15.7082,   6.0323,   0.0995, -13.2788,\n",
      "          6.1239,   0.1551], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  34\n",
      "reference_tensor:  tensor([-16.9111,   6.1213,   0.0413, -14.4590,   6.1394,   0.0466, -11.9993,\n",
      "          6.1593,   0.0509], device='cuda:0')\n",
      "decoder_prediction:  tensor([-16.6840,   5.9323,   0.0233, -14.2729,   5.9912,   0.0751, -11.8634,\n",
      "          6.0781,   0.1391], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  35\n",
      "reference_tensor:  tensor([-15.6860,   6.1301,   0.0440, -13.2302,   6.1492,   0.0489, -10.7664,\n",
      "          6.1699,   0.0526], device='cuda:0')\n",
      "decoder_prediction:  tensor([-1.5355e+01,  5.9018e+00, -7.8030e-03, -1.2954e+01,  5.9530e+00,\n",
      "         5.3649e-02, -1.0563e+01,  6.0358e+00,  1.2509e-01], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  36\n",
      "reference_tensor:  tensor([-14.4590,   6.1394,   0.0466, -11.9993,   6.1593,   0.0509,  -9.5314,\n",
      "          6.1807,   0.0540], device='cuda:0')\n",
      "decoder_prediction:  tensor([-13.9158,   5.8791,  -0.0496, -11.5212,   5.9192,   0.0240,  -9.1469,\n",
      "          5.9952,   0.1048], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  37\n",
      "reference_tensor:  tensor([-13.2302,   6.1492,   0.0489, -10.7664,   6.1699,   0.0526,  -8.2941,\n",
      "          6.1917,   0.0552], device='cuda:0')\n",
      "decoder_prediction:  tensor([-1.3024e+01,  5.9552e+00, -5.4048e-03, -1.0597e+01,  6.0092e+00,\n",
      "         6.1925e-02, -8.1854e+00,  6.0972e+00,  1.3755e-01], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  38\n",
      "reference_tensor:  tensor([-11.9993,   6.1593,   0.0509,  -9.5314,   6.1807,   0.0540,  -7.0547,\n",
      "          6.2029,   0.0561], device='cuda:0')\n",
      "decoder_prediction:  tensor([-11.9476,   6.1112,   0.0616,  -9.4555,   6.1862,   0.1207,  -6.9696,\n",
      "          6.2911,   0.1863], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  39\n",
      "reference_tensor:  tensor([-10.7664,   6.1699,   0.0526,  -8.2941,   6.1917,   0.0552,  -5.8130,\n",
      "          6.2143,   0.0569], device='cuda:0')\n",
      "decoder_prediction:  tensor([-10.7771,   6.1778,   0.1023,  -8.2556,   6.2669,   0.1603,  -5.7347,\n",
      "          6.3836,   0.2205], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  40\n",
      "reference_tensor:  tensor([-9.5314,  6.1807,  0.0540, -7.0547,  6.2029,  0.0561, -4.5689,  6.2258,\n",
      "         0.0575], device='cuda:0')\n",
      "decoder_prediction:  tensor([-9.5771,  6.2091,  0.0769, -7.0435,  6.2909,  0.1425, -4.5144,  6.4034,\n",
      "         0.2094], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  41\n",
      "reference_tensor:  tensor([-8.2941,  6.1917,  0.0552, -5.8130,  6.2143,  0.0569, -3.3226,  6.2374,\n",
      "         0.0580], device='cuda:0')\n",
      "decoder_prediction:  tensor([-8.3717,  6.2332,  0.0142, -5.8310,  6.2957,  0.0921, -3.3037,  6.3944,\n",
      "         0.1724], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  42\n",
      "reference_tensor:  tensor([-7.0547,  6.2029,  0.0561, -4.5689,  6.2258,  0.0575, -2.0740,  6.2491,\n",
      "         0.0584], device='cuda:0')\n",
      "decoder_prediction:  tensor([-7.1656,  6.2500, -0.0426, -4.6200,  6.2953,  0.0478, -2.0960,  6.3819,\n",
      "         0.1401], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  43\n",
      "reference_tensor:  tensor([-5.8130,  6.2143,  0.0569, -3.3226,  6.2374,  0.0580, -0.8230,  6.2609,\n",
      "         0.0587], device='cuda:0')\n",
      "decoder_prediction:  tensor([-5.9056,  6.2446, -0.0711, -3.3627,  6.2824,  0.0291, -0.8448,  6.3645,\n",
      "         0.1288], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  44\n",
      "reference_tensor:  tensor([-4.5689,  6.2258,  0.0575, -2.0740,  6.2491,  0.0584,  0.4304,  6.2726,\n",
      "         0.0590], device='cuda:0')\n",
      "decoder_prediction:  tensor([-4.6082,  6.2370, -0.0887, -2.0688,  6.2705,  0.0196,  0.4443,  6.3510,\n",
      "         0.1252], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  45\n",
      "reference_tensor:  tensor([-3.3226,  6.2374,  0.0580, -0.8230,  6.2609,  0.0587,  1.6861,  6.2845,\n",
      "         0.0591], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.2871,  6.2337, -0.0954, -0.7496,  6.2664,  0.0191,  1.7625,  6.3478,\n",
      "         0.1293], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  46\n",
      "reference_tensor:  tensor([-2.0740,  6.2491,  0.0584,  0.4304,  6.2726,  0.0590,  2.9442,  6.2963,\n",
      "         0.0592], device='cuda:0')\n",
      "decoder_prediction:  tensor([-1.9564,  6.2376, -0.0929,  0.5822,  6.2723,  0.0264,  3.0978,  6.3568,\n",
      "         0.1396], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  47\n",
      "reference_tensor:  tensor([-0.8230,  6.2609,  0.0587,  1.6861,  6.2845,  0.0591,  4.2046,  6.3082,\n",
      "         0.0593], device='cuda:0')\n",
      "decoder_prediction:  tensor([-0.6216,  6.2353, -0.0404,  1.9184,  6.2885,  0.0784,  4.4454,  6.3891,\n",
      "         0.1856], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  48\n",
      "reference_tensor:  tensor([0.4304, 6.2726, 0.0590, 2.9442, 6.2963, 0.0592, 5.4674, 6.3201, 0.0594],\n",
      "       device='cuda:0')\n",
      "decoder_prediction:  tensor([0.6958, 6.2374, 0.0268, 3.2397, 6.3140, 0.1434, 5.7829, 6.4342, 0.2418],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  49\n",
      "reference_tensor:  tensor([1.6861, 6.2845, 0.0591, 4.2046, 6.3082, 0.0593, 6.7326, 6.3319, 0.0594],\n",
      "       device='cuda:0')\n",
      "decoder_prediction:  tensor([1.9779, 6.2490, 0.0877, 4.5292, 6.3469, 0.2026, 7.0910, 6.4849, 0.2931],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  50\n",
      "reference_tensor:  tensor([2.9442, 6.2963, 0.0592, 5.4674, 6.3201, 0.0594, 8.0002, 6.3438, 0.0594],\n",
      "       device='cuda:0')\n",
      "decoder_prediction:  tensor([3.2101, 6.2723, 0.1288, 5.7724, 6.3848, 0.2439, 8.3532, 6.5353, 0.3299],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  0\n",
      "reference_tensor:  tensor([-47.9365,  10.3087,   0.2177, -43.7962,  10.3923,   0.2062, -39.6234,\n",
      "         10.4715,   0.1954], device='cuda:0')\n",
      "decoder_prediction:  tensor([-47.8536,  10.5150,   0.6752, -43.6012,  10.7043,   0.4224, -39.3188,\n",
      "         10.8270,   0.2610], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  1\n",
      "reference_tensor:  tensor([-45.8706,  10.3511,   0.2118, -41.7138,  10.4324,   0.2007, -37.5252,\n",
      "         10.5096,   0.1902], device='cuda:0')\n",
      "decoder_prediction:  tensor([-45.9869,  10.4026,   0.4049, -41.7842,  10.5110,   0.2083, -37.5921,\n",
      "         10.5717,   0.0918], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  2\n",
      "reference_tensor:  tensor([-43.7962,  10.3923,   0.2062, -39.6234,  10.4715,   0.1954, -35.4196,\n",
      "         10.5466,   0.1853], device='cuda:0')\n",
      "decoder_prediction:  tensor([-4.3868e+01,  1.0413e+01,  2.6905e-01, -3.9668e+01,  1.0481e+01,\n",
      "         1.0589e-01, -3.5491e+01,  1.0511e+01,  1.5362e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  3\n",
      "reference_tensor:  tensor([-41.7138,  10.4324,   0.2007, -37.5252,  10.5096,   0.1902, -33.3067,\n",
      "         10.5827,   0.1806], device='cuda:0')\n",
      "decoder_prediction:  tensor([-4.1774e+01,  1.0458e+01,  2.0211e-01, -3.7564e+01,  1.0506e+01,\n",
      "         6.0047e-02, -3.3375e+01,  1.0522e+01, -1.4752e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  4\n",
      "reference_tensor:  tensor([-39.6234,  10.4715,   0.1954, -35.4196,  10.5466,   0.1853, -31.1866,\n",
      "         10.6179,   0.1760], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.9669e+01,  1.0496e+01,  1.6446e-01, -3.5451e+01,  1.0533e+01,\n",
      "         3.8581e-02, -3.1248e+01,  1.0542e+01, -2.4559e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  5\n",
      "reference_tensor:  tensor([-37.5252,  10.5096,   0.1902, -33.3067,  10.5827,   0.1806, -29.0596,\n",
      "         10.6523,   0.1717], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.7551e+01,  1.0525e+01,  1.4112e-01, -3.3327e+01,  1.0556e+01,\n",
      "         2.9095e-02, -2.9111e+01,  1.0562e+01, -2.4310e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  6\n",
      "reference_tensor:  tensor([-35.4196,  10.5466,   0.1853, -31.1866,  10.6179,   0.1760, -26.9258,\n",
      "         10.6858,   0.1675], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.5432e+01,  1.0553e+01,  1.2538e-01, -3.1203e+01,  1.0581e+01,\n",
      "         2.5792e-02, -2.6973e+01,  1.0586e+01, -1.9020e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  7\n",
      "reference_tensor:  tensor([-33.3067,  10.5827,   0.1806, -29.0596,  10.6523,   0.1717, -24.7854,\n",
      "         10.7185,   0.1634], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.3353e+01,  1.0648e+01,  1.2489e-01, -2.9088e+01,  1.0676e+01,\n",
      "         3.2187e-02, -2.4819e+01,  1.0684e+01, -7.9804e-03], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  8\n",
      "reference_tensor:  tensor([-31.1866,  10.6179,   0.1760, -26.9258,  10.6858,   0.1675, -22.6385,\n",
      "         10.7504,   0.1595], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.1230e+01,  1.0694e+01,  1.3538e-01, -2.6946e+01,  1.0727e+01,\n",
      "         4.7969e-02, -2.2655e+01,  1.0741e+01,  1.1187e-02], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  9\n",
      "reference_tensor:  tensor([-29.0596,  10.6523,   0.1717, -24.7854,  10.7185,   0.1634, -20.4853,\n",
      "         10.7815,   0.1558], device='cuda:0')\n",
      "decoder_prediction:  tensor([-29.0955,  10.7140,   0.1585, -24.8025,  10.7562,   0.0743, -20.4983,\n",
      "         10.7803,   0.0393], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  10\n",
      "reference_tensor:  tensor([-26.9258,  10.6858,   0.1675, -22.6385,  10.7504,   0.1595, -18.3260,\n",
      "         10.8120,   0.1522], device='cuda:0')\n",
      "decoder_prediction:  tensor([-26.9789,  10.7244,   0.1943, -22.6800,  10.7796,   0.1111, -18.3643,\n",
      "         10.8169,   0.0760], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  11\n",
      "reference_tensor:  tensor([-24.7854,  10.7185,   0.1634, -20.4853,  10.7815,   0.1558, -16.1606,\n",
      "         10.8417,   0.1487], device='cuda:0')\n",
      "decoder_prediction:  tensor([-24.8179,  10.7149,   0.1242, -20.5241,  10.7505,   0.0615, -16.2226,\n",
      "         10.7749,   0.0421], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  12\n",
      "reference_tensor:  tensor([-22.6385,  10.7504,   0.1595, -18.3260,  10.8120,   0.1522, -13.9893,\n",
      "         10.8708,   0.1454], device='cuda:0')\n",
      "decoder_prediction:  tensor([-22.6145,  10.7631,   0.0986, -18.3018,  10.7924,   0.0478, -13.9842,\n",
      "         10.8141,   0.0371], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  13\n",
      "reference_tensor:  tensor([-20.4853,  10.7815,   0.1558, -16.1606,  10.8417,   0.1487, -11.8123,\n",
      "         10.8993,   0.1423], device='cuda:0')\n",
      "decoder_prediction:  tensor([-20.4502,  10.8097,   0.0951, -16.1187,  10.8395,   0.0521, -11.7818,\n",
      "         10.8642,   0.0469], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  14\n",
      "reference_tensor:  tensor([-18.3260,  10.8120,   0.1522, -13.9893,  10.8708,   0.1454,  -9.6297,\n",
      "         10.9271,   0.1393], device='cuda:0')\n",
      "decoder_prediction:  tensor([-18.3103,  10.8465,   0.0991, -13.9639,  10.8792,   0.0627,  -9.6102,\n",
      "         10.9087,   0.0620], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  15\n",
      "reference_tensor:  tensor([-16.1606,  10.8417,   0.1487, -11.8123,  10.8993,   0.1423,  -7.4416,\n",
      "         10.9544,   0.1364], device='cuda:0')\n",
      "decoder_prediction:  tensor([-16.1902,  10.8782,   0.1040, -11.8308,  10.9142,   0.0741,  -7.4621,\n",
      "         10.9487,   0.0777], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  16\n",
      "reference_tensor:  tensor([-13.9893,  10.8708,   0.1454,  -9.6297,  10.9271,   0.1393,  -5.2480,\n",
      "         10.9811,   0.1336], device='cuda:0')\n",
      "decoder_prediction:  tensor([-14.0573,  10.9035,   0.0988,  -9.6868,  10.9397,   0.0769,  -5.3081,\n",
      "         10.9771,   0.0862], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  17\n",
      "reference_tensor:  tensor([-11.8123,  10.8993,   0.1423,  -7.4416,  10.9544,   0.1364,  -3.0492,\n",
      "         11.0073,   0.1309], device='cuda:0')\n",
      "decoder_prediction:  tensor([-11.8262,  10.9353,   0.0947,  -7.4388,  10.9724,   0.0796,  -3.0493,\n",
      "         11.0135,   0.0939], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  18\n",
      "reference_tensor:  tensor([-9.6297, 10.9271,  0.1393, -5.2480, 10.9811,  0.1336, -0.8452, 11.0329,\n",
      "         0.1283], device='cuda:0')\n",
      "decoder_prediction:  tensor([-9.6411, 10.9448,  0.1095, -5.2465, 10.9887,  0.0987, -0.8509, 11.0383,\n",
      "         0.1156], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  19\n",
      "reference_tensor:  tensor([-7.4416, 10.9544,  0.1364, -3.0492, 11.0073,  0.1309,  1.3639, 11.0581,\n",
      "         0.1257], device='cuda:0')\n",
      "decoder_prediction:  tensor([-7.4680, 10.9521,  0.1277, -3.0679, 11.0039,  0.1208,  1.3343, 11.0626,\n",
      "         0.1402], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  20\n",
      "reference_tensor:  tensor([-5.2480, 10.9811,  0.1336, -0.8452, 11.0329,  0.1283,  3.5780, 11.0827,\n",
      "         0.1232], device='cuda:0')\n",
      "decoder_prediction:  tensor([-5.2800, 10.9624,  0.1428, -0.8742, 11.0209,  0.1406,  3.5357, 11.0880,\n",
      "         0.1630], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  21\n",
      "reference_tensor:  tensor([-3.0492, 11.0073,  0.1309,  1.3639, 11.0581,  0.1257,  5.7970, 11.1069,\n",
      "         0.1208], device='cuda:0')\n",
      "decoder_prediction:  tensor([-3.0718, 10.9789,  0.1517,  1.3416, 11.0422,  0.1556,  5.7609, 11.1157,\n",
      "         0.1820], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  22\n",
      "reference_tensor:  tensor([-0.8452, 11.0329,  0.1283,  3.5780, 11.0827,  0.1232,  8.0207, 11.1305,\n",
      "         0.1184], device='cuda:0')\n",
      "decoder_prediction:  tensor([-0.8517, 11.0030,  0.1510,  3.5715, 11.0679,  0.1628,  8.0019, 11.1455,\n",
      "         0.1946], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  23\n",
      "reference_tensor:  tensor([ 1.3639, 11.0581,  0.1257,  5.7970, 11.1069,  0.1208, 10.2491, 11.1538,\n",
      "         0.1161], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 1.3648, 11.0346,  0.1360,  5.8002, 11.0966,  0.1583, 10.2422, 11.1745,\n",
      "         0.1976], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  24\n",
      "reference_tensor:  tensor([ 3.5780, 11.0827,  0.1232,  8.0207, 11.1305,  0.1184, 12.4822, 11.1765,\n",
      "         0.1137], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 3.5631, 11.0711,  0.1013,  8.0120, 11.1241,  0.1374, 12.4643, 11.1972,\n",
      "         0.1871], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "time_step:  25\n",
      "reference_tensor:  tensor([ 5.7970, 11.1069,  0.1208, 10.2491, 11.1538,  0.1161, 14.7197, 11.1988,\n",
      "         0.1114], device='cuda:0')\n",
      "decoder_prediction:  tensor([ 5.7338, 11.1070,  0.0427, 10.1956, 11.1434,  0.0967, 14.6537, 11.2057,\n",
      "         0.1600], device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the network\n",
    "\n",
    "####### Load the neural network weights from saved models #######\n",
    "\n",
    "loaded_network=NN_structure(n_epochs,min_timesteps,n_rollout,n_skips_per_rollout,n_test,n_input,n_output,n_state_enc,learning_rate,gen_model,pred_model,device)\n",
    "loaded_network.load_model_weights(\"encoder_rnn_decoder_simple_nn_n_rollout_3\")\n",
    "\n",
    "#test_keys=[keys_for_testing[15]]\n",
    "test_results=loaded_network.test(503)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([-24.064964  ,   4.1906233 ,   0.67194194], dtype=float32),\n",
       "  array([-25.229195  ,   3.1972115 ,   0.36127877, -23.918798  ,\n",
       "           3.4208672 ,   0.4649216 , -22.453152  ,   3.6634502 ,\n",
       "           0.5251166 ], dtype=float32)],\n",
       " [array([-23.21346   ,   4.324399  ,   0.66887933], dtype=float32),\n",
       "  array([-2.4573174e+01,  4.1392016e+00, -1.2943864e-02, -2.2882221e+01,\n",
       "          4.2043829e+00,  6.4425588e-02, -2.1170528e+01,  4.3009992e+00,\n",
       "          1.4079911e-01], dtype=float32)],\n",
       " [array([-22.335272 ,   4.457504 ,   0.6655236], dtype=float32),\n",
       "  array([-2.4191008e+01,  4.4817510e+00,  2.3482800e-02, -2.2353559e+01,\n",
       "          4.5476985e+00,  6.8805099e-02, -2.0519100e+01,  4.6405687e+00,\n",
       "          1.2878224e-01], dtype=float32)],\n",
       " [array([-21.430532 ,   4.589883 ,   0.6618953], dtype=float32),\n",
       "  array([-23.419863  ,   4.531012  ,   0.2751075 , -21.544128  ,\n",
       "           4.6796565 ,   0.2870773 , -19.645433  ,   4.8382454 ,\n",
       "           0.30386758], dtype=float32)],\n",
       " [array([-20.499395 ,   4.7214856,   0.6580137], dtype=float32),\n",
       "  array([-22.509552  ,   4.5260644 ,   0.52183104, -20.617292  ,\n",
       "           4.757416  ,   0.5067407 , -18.673653  ,   4.9814906 ,\n",
       "           0.48048532], dtype=float32)],\n",
       " [array([-19.54202   ,   4.852265  ,   0.65389687], dtype=float32),\n",
       "  array([-21.550707  ,   4.5461373 ,   0.69244313, -19.637623  ,\n",
       "           4.834811  ,   0.6596526 , -17.653362  ,   5.104507  ,\n",
       "           0.604138  ], dtype=float32)],\n",
       " [array([-18.558577  ,   4.9821773 ,   0.64956164], dtype=float32),\n",
       "  array([-20.565636 ,   4.594119 ,   0.8154297, -18.62436  ,   4.924078 ,\n",
       "           0.7705902, -16.597326 ,   5.226753 ,   0.6943649], dtype=float32)],\n",
       " [array([-17.54924  ,   5.111182 ,   0.6450239], dtype=float32),\n",
       "  array([-19.56216   ,   4.666369  ,   0.90280044, -17.585802  ,\n",
       "           5.025587  ,   0.85006297, -15.513084  ,   5.351741  ,\n",
       "           0.75947976], dtype=float32)],\n",
       " [array([-16.514198  ,   5.2392416 ,   0.64029837], dtype=float32),\n",
       "  array([-18.542923  ,   4.758557  ,   0.9604596 , -16.525774  ,\n",
       "           5.137016  ,   0.9032719 , -14.405105  ,   5.478742  ,\n",
       "           0.80360925], dtype=float32)],\n",
       " [array([-15.453642  ,   5.3663216 ,   0.63539916], dtype=float32),\n",
       "  array([-17.508823 ,   4.867    ,   0.9918889, -15.44628  ,   5.2558837,\n",
       "           0.9333004, -13.276245 ,   5.6062374,   0.8292098], dtype=float32)],\n",
       " [array([-14.36777  ,   5.4923897,   0.6303392], dtype=float32),\n",
       "  array([-16.460186  ,   4.9888053 ,   0.9995192 , -14.348541  ,\n",
       "           5.38014   ,   0.94230866, -12.128435  ,   5.7328563 ,\n",
       "           0.8380054 ], dtype=float32)],\n",
       " [array([-13.25679   ,   5.6174154 ,   0.62513095], dtype=float32),\n",
       "  array([-15.397188  ,   5.121688  ,   0.98524725, -13.23346   ,\n",
       "           5.508154  ,   0.9319823 , -10.963144  ,   5.857495  ,\n",
       "           0.8313422 ], dtype=float32)],\n",
       " [array([-12.120912 ,   5.7413726,   0.6197859], dtype=float32),\n",
       "  array([-14.320077 ,   5.263815 ,   0.9506782, -12.101856 ,   5.6386495,\n",
       "           0.9037502,  -9.781624 ,   5.9793224,   0.8103659], dtype=float32)],\n",
       " [array([-10.96035 ,   5.864236,   0.614315], dtype=float32),\n",
       "  array([-13.229162  ,   5.413659  ,   0.8972155 , -10.954523  ,\n",
       "           5.770581  ,   0.8588618 ,  -8.585015  ,   6.09768   ,\n",
       "           0.77607656], dtype=float32)],\n",
       " [array([-9.775329  ,  5.9859815 ,  0.60872847], dtype=float32),\n",
       "  array([-12.124909  ,   5.5699105 ,   0.8261254 ,  -9.792342  ,\n",
       "           5.9030695 ,   0.79844165,  -7.374478  ,   6.2120323 ,\n",
       "           0.72937715], dtype=float32)],\n",
       " [array([-8.5660715 ,  6.106589  ,  0.60303605], dtype=float32),\n",
       "  array([-11.007891  ,   5.731391  ,   0.7385547 ,  -8.616266  ,\n",
       "           6.035331  ,   0.7235124 ,  -6.151212  ,   6.321906  ,\n",
       "           0.67108834], dtype=float32)],\n",
       " [array([-7.332809  ,  6.226038  ,  0.59724694], dtype=float32),\n",
       "  array([-9.87882  ,  5.897031 ,  0.6355704, -7.427352 ,  6.1666584,\n",
       "          0.6350225, -4.916497 ,  6.4268856,  0.6019731], dtype=float32)],\n",
       " [array([-6.0757737,  6.344312 ,  0.5913698], dtype=float32),\n",
       "  array([-8.738549  ,  6.0635176 ,  0.52366436, -6.2274203 ,  6.295902  ,\n",
       "          0.5387356 , -3.6721563 ,  6.5275164 ,  0.5266715 ], dtype=float32)],\n",
       " [array([-4.795203 ,  6.461395 ,  0.5854127], dtype=float32),\n",
       "  array([-7.5773005 ,  6.211478  ,  0.4484775 , -5.012059  ,  6.4188657 ,\n",
       "          0.4752047 , -2.4122143 ,  6.6314855 ,  0.47787553], dtype=float32)],\n",
       " [array([-3.4913366 ,  6.5772715 ,  0.57938343], dtype=float32),\n",
       "  array([-6.366022  ,  6.3444414 ,  0.40340924, -3.7511656 ,  6.536907  ,\n",
       "          0.43869162, -1.1064603 ,  6.7384796 ,  0.45104903], dtype=float32)],\n",
       " [array([-2.1644166,  6.6919293,  0.5732893], dtype=float32),\n",
       "  array([-5.099572  ,  6.466223  ,  0.38826835, -2.4381092 ,  6.6537437 ,\n",
       "          0.42898166,  0.25319642,  6.8521237 ,  0.44599152], dtype=float32)],\n",
       " [array([-0.814688 ,  6.8053565,  0.5671372], dtype=float32),\n",
       "  array([-3.782734  ,  6.579938  ,  0.4021728 , -1.0764835 ,  6.772164  ,\n",
       "          0.44522077,  1.6641917 ,  6.9749236 ,  0.46196562], dtype=float32)],\n",
       " [array([0.557602  , 6.9175434 , 0.56093365], dtype=float32),\n",
       "  array([-2.4262767 ,  6.6885633 ,  0.4412719 ,  0.32393262,  6.89384   ,\n",
       "          0.48391467,  3.1171212 ,  7.107514  ,  0.49610364], dtype=float32)],\n",
       " [array([1.9522043 , 7.0284805 , 0.55468494], dtype=float32),\n",
       "  array([-1.045743  ,  6.795353  ,  0.4980222 ,  1.7485218 ,  7.0195036 ,\n",
       "          0.53829515,  4.5970984 ,  7.2486377 ,  0.5428985 ], dtype=float32)],\n",
       " [array([3.3688684 , 7.1381598 , 0.54839694], dtype=float32),\n",
       "  array([0.339579  , 6.903931  , 0.56118965, 3.1788898 , 7.1490417 ,\n",
       "         0.59831893, 6.084859  , 7.3952394 , 0.5942094 ], dtype=float32)],\n",
       " [array([4.8073416 , 7.246575  , 0.54207516], dtype=float32),\n",
       "  array([1.7082402 , 7.018051  , 0.61644554, 4.594379  , 7.2814507 ,\n",
       "         0.6512085 , 7.5583067 , 7.542589  , 0.63969827], dtype=float32)],\n",
       " [array([6.267371  , 7.3537197 , 0.53572494], dtype=float32),\n",
       "  array([3.0390005 , 7.141148  , 0.64757097, 5.974329  , 7.414793  ,\n",
       "         0.68252873, 8.994834  , 7.684547  , 0.6677048 ], dtype=float32)],\n",
       " [array([-24.064964  ,   4.1906233 ,   0.67194194], dtype=float32),\n",
       "  array([-25.229195  ,   3.1972115 ,   0.36127877, -23.918798  ,\n",
       "           3.4208672 ,   0.4649216 , -22.453152  ,   3.6634502 ,\n",
       "           0.5251166 ], dtype=float32)],\n",
       " [array([-23.21346   ,   4.324399  ,   0.66887933], dtype=float32),\n",
       "  array([-2.4573174e+01,  4.1392016e+00, -1.2943864e-02, -2.2882221e+01,\n",
       "          4.2043829e+00,  6.4425588e-02, -2.1170528e+01,  4.3009992e+00,\n",
       "          1.4079911e-01], dtype=float32)],\n",
       " [array([-22.335272 ,   4.457504 ,   0.6655236], dtype=float32),\n",
       "  array([-2.4191008e+01,  4.4817510e+00,  2.3482800e-02, -2.2353559e+01,\n",
       "          4.5476985e+00,  6.8805099e-02, -2.0519100e+01,  4.6405687e+00,\n",
       "          1.2878224e-01], dtype=float32)],\n",
       " [array([-21.430532 ,   4.589883 ,   0.6618953], dtype=float32),\n",
       "  array([-23.419863  ,   4.531012  ,   0.2751075 , -21.544128  ,\n",
       "           4.6796565 ,   0.2870773 , -19.645433  ,   4.8382454 ,\n",
       "           0.30386758], dtype=float32)],\n",
       " [array([-20.499395 ,   4.7214856,   0.6580137], dtype=float32),\n",
       "  array([-22.509552  ,   4.5260644 ,   0.52183104, -20.617292  ,\n",
       "           4.757416  ,   0.5067407 , -18.673653  ,   4.9814906 ,\n",
       "           0.48048532], dtype=float32)],\n",
       " [array([-19.54202   ,   4.852265  ,   0.65389687], dtype=float32),\n",
       "  array([-21.550707  ,   4.5461373 ,   0.69244313, -19.637623  ,\n",
       "           4.834811  ,   0.6596526 , -17.653362  ,   5.104507  ,\n",
       "           0.604138  ], dtype=float32)],\n",
       " [array([-18.558577  ,   4.9821773 ,   0.64956164], dtype=float32),\n",
       "  array([-20.565636 ,   4.594119 ,   0.8154297, -18.62436  ,   4.924078 ,\n",
       "           0.7705902, -16.597326 ,   5.226753 ,   0.6943649], dtype=float32)],\n",
       " [array([-17.54924  ,   5.111182 ,   0.6450239], dtype=float32),\n",
       "  array([-19.56216   ,   4.666369  ,   0.90280044, -17.585802  ,\n",
       "           5.025587  ,   0.85006297, -15.513084  ,   5.351741  ,\n",
       "           0.75947976], dtype=float32)],\n",
       " [array([-16.514198  ,   5.2392416 ,   0.64029837], dtype=float32),\n",
       "  array([-18.542923  ,   4.758557  ,   0.9604596 , -16.525774  ,\n",
       "           5.137016  ,   0.9032719 , -14.405105  ,   5.478742  ,\n",
       "           0.80360925], dtype=float32)],\n",
       " [array([-15.453642  ,   5.3663216 ,   0.63539916], dtype=float32),\n",
       "  array([-17.508823 ,   4.867    ,   0.9918889, -15.44628  ,   5.2558837,\n",
       "           0.9333004, -13.276245 ,   5.6062374,   0.8292098], dtype=float32)],\n",
       " [array([-14.36777  ,   5.4923897,   0.6303392], dtype=float32),\n",
       "  array([-16.460186  ,   4.9888053 ,   0.9995192 , -14.348541  ,\n",
       "           5.38014   ,   0.94230866, -12.128435  ,   5.7328563 ,\n",
       "           0.8380054 ], dtype=float32)],\n",
       " [array([-13.25679   ,   5.6174154 ,   0.62513095], dtype=float32),\n",
       "  array([-15.397188  ,   5.121688  ,   0.98524725, -13.23346   ,\n",
       "           5.508154  ,   0.9319823 , -10.963144  ,   5.857495  ,\n",
       "           0.8313422 ], dtype=float32)],\n",
       " [array([-12.120912 ,   5.7413726,   0.6197859], dtype=float32),\n",
       "  array([-14.320077 ,   5.263815 ,   0.9506782, -12.101856 ,   5.6386495,\n",
       "           0.9037502,  -9.781624 ,   5.9793224,   0.8103659], dtype=float32)],\n",
       " [array([-10.96035 ,   5.864236,   0.614315], dtype=float32),\n",
       "  array([-13.229162  ,   5.413659  ,   0.8972155 , -10.954523  ,\n",
       "           5.770581  ,   0.8588618 ,  -8.585015  ,   6.09768   ,\n",
       "           0.77607656], dtype=float32)],\n",
       " [array([-9.775329  ,  5.9859815 ,  0.60872847], dtype=float32),\n",
       "  array([-12.124909  ,   5.5699105 ,   0.8261254 ,  -9.792342  ,\n",
       "           5.9030695 ,   0.79844165,  -7.374478  ,   6.2120323 ,\n",
       "           0.72937715], dtype=float32)],\n",
       " [array([-8.5660715 ,  6.106589  ,  0.60303605], dtype=float32),\n",
       "  array([-11.007891  ,   5.731391  ,   0.7385547 ,  -8.616266  ,\n",
       "           6.035331  ,   0.7235124 ,  -6.151212  ,   6.321906  ,\n",
       "           0.67108834], dtype=float32)],\n",
       " [array([-7.332809  ,  6.226038  ,  0.59724694], dtype=float32),\n",
       "  array([-9.87882  ,  5.897031 ,  0.6355704, -7.427352 ,  6.1666584,\n",
       "          0.6350225, -4.916497 ,  6.4268856,  0.6019731], dtype=float32)],\n",
       " [array([-6.0757737,  6.344312 ,  0.5913698], dtype=float32),\n",
       "  array([-8.738549  ,  6.0635176 ,  0.52366436, -6.2274203 ,  6.295902  ,\n",
       "          0.5387356 , -3.6721563 ,  6.5275164 ,  0.5266715 ], dtype=float32)],\n",
       " [array([-4.795203 ,  6.461395 ,  0.5854127], dtype=float32),\n",
       "  array([-7.5773005 ,  6.211478  ,  0.4484775 , -5.012059  ,  6.4188657 ,\n",
       "          0.4752047 , -2.4122143 ,  6.6314855 ,  0.47787553], dtype=float32)],\n",
       " [array([-3.4913366 ,  6.5772715 ,  0.57938343], dtype=float32),\n",
       "  array([-6.366022  ,  6.3444414 ,  0.40340924, -3.7511656 ,  6.536907  ,\n",
       "          0.43869162, -1.1064603 ,  6.7384796 ,  0.45104903], dtype=float32)],\n",
       " [array([-2.1644166,  6.6919293,  0.5732893], dtype=float32),\n",
       "  array([-5.099572  ,  6.466223  ,  0.38826835, -2.4381092 ,  6.6537437 ,\n",
       "          0.42898166,  0.25319642,  6.8521237 ,  0.44599152], dtype=float32)],\n",
       " [array([-0.814688 ,  6.8053565,  0.5671372], dtype=float32),\n",
       "  array([-3.782734  ,  6.579938  ,  0.4021728 , -1.0764835 ,  6.772164  ,\n",
       "          0.44522077,  1.6641917 ,  6.9749236 ,  0.46196562], dtype=float32)],\n",
       " [array([0.557602  , 6.9175434 , 0.56093365], dtype=float32),\n",
       "  array([-2.4262767 ,  6.6885633 ,  0.4412719 ,  0.32393262,  6.89384   ,\n",
       "          0.48391467,  3.1171212 ,  7.107514  ,  0.49610364], dtype=float32)],\n",
       " [array([1.9522043 , 7.0284805 , 0.55468494], dtype=float32),\n",
       "  array([-1.045743  ,  6.795353  ,  0.4980222 ,  1.7485218 ,  7.0195036 ,\n",
       "          0.53829515,  4.5970984 ,  7.2486377 ,  0.5428985 ], dtype=float32)],\n",
       " [array([3.3688684 , 7.1381598 , 0.54839694], dtype=float32),\n",
       "  array([0.339579  , 6.903931  , 0.56118965, 3.1788898 , 7.1490417 ,\n",
       "         0.59831893, 6.084859  , 7.3952394 , 0.5942094 ], dtype=float32)],\n",
       " [array([4.8073416 , 7.246575  , 0.54207516], dtype=float32),\n",
       "  array([1.7082402 , 7.018051  , 0.61644554, 4.594379  , 7.2814507 ,\n",
       "         0.6512085 , 7.5583067 , 7.542589  , 0.63969827], dtype=float32)],\n",
       " [array([6.267371  , 7.3537197 , 0.53572494], dtype=float32),\n",
       "  array([3.0390005 , 7.141148  , 0.64757097, 5.974329  , 7.414793  ,\n",
       "         0.68252873, 8.994834  , 7.684547  , 0.6677048 ], dtype=float32)],\n",
       " [array([-38.64622   ,   8.290066  ,  -0.13579828], dtype=float32),\n",
       "  array([-41.529644  ,   8.766684  ,   0.3201648 , -37.996727  ,\n",
       "           8.866679  ,   0.18731868, -34.442604  ,   8.935522  ,\n",
       "           0.11616442], dtype=float32)],\n",
       " [array([-36.99102   ,   8.261925  ,  -0.14070557], dtype=float32),\n",
       "  array([-4.0442463e+01,  8.5590897e+00,  1.7748415e-01, -3.6989151e+01,\n",
       "          8.6184196e+00,  7.4838996e-02, -3.3544216e+01,  8.6577349e+00,\n",
       "          2.8103441e-02], dtype=float32)],\n",
       " [array([-35.341545  ,   8.232849  ,  -0.14538139], dtype=float32),\n",
       "  array([-3.8683575e+01,  8.3278570e+00,  6.6821814e-02, -3.5325733e+01,\n",
       "          8.3565941e+00, -5.6546926e-03, -3.1988209e+01,  8.3749056e+00,\n",
       "         -2.9109925e-02], dtype=float32)],\n",
       " [array([-33.697968  ,   8.2029    ,  -0.14974135], dtype=float32),\n",
       "  array([-3.6932312e+01,  8.2368269e+00, -2.2686362e-02, -3.3614243e+01,\n",
       "          8.2401533e+00, -7.0757270e-02, -3.0325340e+01,  8.2406607e+00,\n",
       "         -7.5443119e-02], dtype=float32)],\n",
       " [array([-32.060463  ,   8.172165  ,  -0.15367597], dtype=float32),\n",
       "  array([-35.26243   ,   8.176774  ,  -0.08131927, -31.972218  ,\n",
       "           8.163786  ,  -0.11081851, -28.713772  ,   8.153425  ,\n",
       "          -0.1015223 ], dtype=float32)],\n",
       " [array([-30.42917   ,   8.140756  ,  -0.15704684], dtype=float32),\n",
       "  array([-33.630306  ,   8.129036  ,  -0.11909628, -30.363354  ,\n",
       "           8.105974  ,  -0.13375843, -27.126427  ,   8.089582  ,\n",
       "          -0.11356804], dtype=float32)],\n",
       " [array([-28.804213  ,   8.108819  ,  -0.15968439], dtype=float32),\n",
       "  array([-32.01709   ,   8.091276  ,  -0.14779639, -28.76943   ,\n",
       "           8.0608225 ,  -0.14937675, -25.54842   ,   8.040454  ,\n",
       "          -0.11972357], dtype=float32)],\n",
       " [array([-27.185677  ,   8.076542  ,  -0.16138643], dtype=float32),\n",
       "  array([-30.401653  ,   8.070909  ,  -0.16565788, -27.165312  ,\n",
       "           8.036341  ,  -0.156654  , -23.952068  ,   8.014594  ,\n",
       "          -0.11946416], dtype=float32)],\n",
       " [array([-25.573606  ,   8.044158  ,  -0.16191941], dtype=float32),\n",
       "  array([-28.78101   ,   8.044665  ,  -0.17404902, -25.557564  ,\n",
       "           8.009031  ,  -0.15623891, -22.353085  ,   7.988457  ,\n",
       "          -0.11291896], dtype=float32)],\n",
       " [array([-23.967995  ,   8.011953  ,  -0.16102396], dtype=float32),\n",
       "  array([-27.158691  ,   8.011062  ,  -0.17381358, -23.950542  ,\n",
       "           7.97714   ,  -0.14873278, -20.75641   ,   7.960057  ,\n",
       "          -0.10051951], dtype=float32)],\n",
       " [array([-22.368773  ,   7.980268  ,  -0.15842536], dtype=float32),\n",
       "  array([-25.538399  ,   7.9729266 ,  -0.16602755, -22.34692   ,\n",
       "           7.9431214 ,  -0.1350497 , -19.163757  ,   7.931518  ,\n",
       "          -0.08304587], dtype=float32)],\n",
       " [array([-20.775797 ,   7.9494977,  -0.1538507], dtype=float32),\n",
       "  array([-23.922638  ,   7.9321675 ,  -0.15201533, -20.748512  ,\n",
       "           7.908454  ,  -0.11630642, -17.576405  ,   7.90395   ,\n",
       "          -0.06144285], dtype=float32)],\n",
       " [array([-19.188837  ,   7.920091  ,  -0.14703375], dtype=float32),\n",
       "  array([-22.31312   ,   7.890009  ,  -0.13312185, -19.156582  ,\n",
       "           7.873928  ,  -0.09363043, -15.995365  ,   7.8777857 ,\n",
       "          -0.03666022], dtype=float32)],\n",
       " [array([-17.607576 ,   7.892531 ,  -0.1377995], dtype=float32),\n",
       "  array([-2.07109871e+01,  7.84739351e+00, -1.10640466e-01, -1.75719242e+01,\n",
       "          7.84007454e+00, -6.81046247e-02, -1.44213028e+01,  7.85321665e+00,\n",
       "         -9.60582495e-03], dtype=float32)],\n",
       " [array([-16.03159   ,   7.867321  ,  -0.12605162], dtype=float32),\n",
       "  array([-1.9117067e+01,  7.8051887e+00, -8.5820615e-02, -1.5995052e+01,\n",
       "          7.8073654e+00, -4.0771842e-02, -1.2854600e+01,  7.8303890e+00,\n",
       "          1.8846899e-02], dtype=float32)],\n",
       " [array([-14.460362  ,   7.8449593 ,  -0.11180731], dtype=float32),\n",
       "  array([-1.75316772e+01,  7.76413345e+00, -5.92207909e-02, -1.44260111e+01,\n",
       "          7.77635956e+00, -1.21036768e-02, -1.12951145e+01,  7.80970716e+00,\n",
       "          4.82997894e-02], dtype=float32)],\n",
       " [array([-12.893274  ,   7.825918  ,  -0.09520522], dtype=float32),\n",
       "  array([-15.959045  ,   7.728193  ,  -0.03084123, -12.867586  ,\n",
       "           7.750967  ,   0.01787639,  -9.74399   ,   7.794998  ,\n",
       "           0.07870471], dtype=float32)],\n",
       " [array([-11.329633  ,   7.810504  ,  -0.07707256], dtype=float32),\n",
       "  array([-1.4396995e+01,  7.6977878e+00, -3.2753944e-03, -1.1317394e+01,\n",
       "          7.7308111e+00,  4.6997070e-02, -8.1992006e+00,  7.7852468e+00,\n",
       "          1.0824427e-01], dtype=float32)],\n",
       " [array([-9.768688  ,  7.79894   , -0.05781847], dtype=float32),\n",
       "  array([-12.843019  ,   7.6718807 ,   0.02078164,  -9.773331  ,\n",
       "           7.714038  ,   0.07303882,  -6.659564  ,   7.7779617 ,\n",
       "           0.13509265], dtype=float32)],\n",
       " [array([-8.209647  ,  7.791471  , -0.03734493], dtype=float32),\n",
       "  array([-11.295309  ,   7.650183  ,   0.03971648,  -8.233734  ,\n",
       "           7.6998644 ,   0.094684  ,  -5.123814  ,   7.7719626 ,\n",
       "           0.15817523], dtype=float32)],\n",
       " [array([-6.6516685 ,  7.7883143 , -0.01578373], dtype=float32),\n",
       "  array([-9.730709  ,  7.626172  ,  0.0222975 , -6.678178  ,  7.672226  ,\n",
       "          0.08620811, -3.5798979 ,  7.743609  ,  0.15629745], dtype=float32)],\n",
       " [array([-5.0938716e+00,  7.7896547e+00,  6.7020166e-03], dtype=float32),\n",
       "  array([-8.1708145 ,  7.6319876 , -0.00921571, -5.114037  ,  7.66993   ,\n",
       "          0.06497896, -2.0193906 ,  7.7369246 ,  0.14299956], dtype=float32)],\n",
       " [array([-3.5353422 ,  7.7956386 ,  0.02992027], dtype=float32),\n",
       "  array([-6.6206646 ,  7.6579533 , -0.01867676, -3.5503697 ,  7.694495  ,\n",
       "          0.06124985, -0.44814688,  7.762432  ,  0.14360172], dtype=float32)],\n",
       " [array([-1.9751413 ,  7.80637   ,  0.05365403], dtype=float32),\n",
       "  array([-5.062315  ,  7.672954  , -0.01851714, -1.9826841 ,  7.7111864 ,\n",
       "          0.06565285,  1.1245508 ,  7.782636  ,  0.15109187], dtype=float32)],\n",
       " [array([-0.412314  ,  7.8219028 ,  0.07766683], dtype=float32),\n",
       "  array([-3.5005937 ,  7.6855073 , -0.01510799, -0.41284445,  7.7264304 ,\n",
       "          0.07290113,  2.6992307 ,  7.802195  ,  0.16105902], dtype=float32)],\n",
       " [array([1.1541008 , 7.842245  , 0.10170949], dtype=float32),\n",
       "  array([-1.9393191 ,  7.701886  , -0.01434815,  1.1576378 ,  7.744607  ,\n",
       "          0.07805187,  4.275832  ,  7.8239093 ,  0.16934836], dtype=float32)],\n",
       " [array([2.7250602 , 7.8673506 , 0.12552767], dtype=float32),\n",
       "  array([-0.38109696,  7.725197  , -0.02141988,  2.7272975 ,  7.7671857 ,\n",
       "          0.07677501,  5.853177  ,  7.847907  ,  0.17233372], dtype=float32)],\n",
       " [array([4.30151   , 7.897144  , 0.14896686], dtype=float32),\n",
       "  array([ 1.1732761 ,  7.7569304 , -0.04052746,  4.2958927 ,  7.7943454 ,\n",
       "          0.06556332,  7.430833  ,  7.8733015 ,  0.16708714], dtype=float32)],\n",
       " [array([5.884378 , 7.9315395, 0.1719793], dtype=float32),\n",
       "  array([ 2.7255697 ,  7.797778  , -0.07468587,  5.8654613 ,  7.8258424 ,\n",
       "          0.04190755,  9.010536  ,  7.899101  ,  0.15152028], dtype=float32)],\n",
       " [array([7.4745774 , 7.9704533 , 0.19456732], dtype=float32),\n",
       "  array([ 4.2799525e+00,  7.8478541e+00, -1.2559724e-01,  7.4402370e+00,\n",
       "          7.8612733e+00,  4.3995380e-03,  1.0596238e+01,  7.9244895e+00,\n",
       "          1.2446809e-01], dtype=float32)],\n",
       " [array([-38.64622   ,   8.290066  ,  -0.13579828], dtype=float32),\n",
       "  array([-41.529644  ,   8.766684  ,   0.3201648 , -37.996727  ,\n",
       "           8.866679  ,   0.18731868, -34.442604  ,   8.935522  ,\n",
       "           0.11616442], dtype=float32)],\n",
       " [array([-36.99102   ,   8.261925  ,  -0.14070557], dtype=float32),\n",
       "  array([-4.0442463e+01,  8.5590897e+00,  1.7748415e-01, -3.6989151e+01,\n",
       "          8.6184196e+00,  7.4838996e-02, -3.3544216e+01,  8.6577349e+00,\n",
       "          2.8103441e-02], dtype=float32)],\n",
       " [array([-35.341545  ,   8.232849  ,  -0.14538139], dtype=float32),\n",
       "  array([-3.8683575e+01,  8.3278570e+00,  6.6821814e-02, -3.5325733e+01,\n",
       "          8.3565941e+00, -5.6546926e-03, -3.1988209e+01,  8.3749056e+00,\n",
       "         -2.9109925e-02], dtype=float32)],\n",
       " [array([-33.697968  ,   8.2029    ,  -0.14974135], dtype=float32),\n",
       "  array([-3.6932312e+01,  8.2368269e+00, -2.2686362e-02, -3.3614243e+01,\n",
       "          8.2401533e+00, -7.0757270e-02, -3.0325340e+01,  8.2406607e+00,\n",
       "         -7.5443119e-02], dtype=float32)],\n",
       " [array([-32.060463  ,   8.172165  ,  -0.15367597], dtype=float32),\n",
       "  array([-35.26243   ,   8.176774  ,  -0.08131927, -31.972218  ,\n",
       "           8.163786  ,  -0.11081851, -28.713772  ,   8.153425  ,\n",
       "          -0.1015223 ], dtype=float32)],\n",
       " [array([-30.42917   ,   8.140756  ,  -0.15704684], dtype=float32),\n",
       "  array([-33.630306  ,   8.129036  ,  -0.11909628, -30.363354  ,\n",
       "           8.105974  ,  -0.13375843, -27.126427  ,   8.089582  ,\n",
       "          -0.11356804], dtype=float32)],\n",
       " [array([-28.804213  ,   8.108819  ,  -0.15968439], dtype=float32),\n",
       "  array([-32.01709   ,   8.091276  ,  -0.14779639, -28.76943   ,\n",
       "           8.0608225 ,  -0.14937675, -25.54842   ,   8.040454  ,\n",
       "          -0.11972357], dtype=float32)],\n",
       " [array([-27.185677  ,   8.076542  ,  -0.16138643], dtype=float32),\n",
       "  array([-30.401653  ,   8.070909  ,  -0.16565788, -27.165312  ,\n",
       "           8.036341  ,  -0.156654  , -23.952068  ,   8.014594  ,\n",
       "          -0.11946416], dtype=float32)],\n",
       " [array([-25.573606  ,   8.044158  ,  -0.16191941], dtype=float32),\n",
       "  array([-28.78101   ,   8.044665  ,  -0.17404902, -25.557564  ,\n",
       "           8.009031  ,  -0.15623891, -22.353085  ,   7.988457  ,\n",
       "          -0.11291896], dtype=float32)],\n",
       " [array([-23.967995  ,   8.011953  ,  -0.16102396], dtype=float32),\n",
       "  array([-27.158691  ,   8.011062  ,  -0.17381358, -23.950542  ,\n",
       "           7.97714   ,  -0.14873278, -20.75641   ,   7.960057  ,\n",
       "          -0.10051951], dtype=float32)],\n",
       " [array([-22.368773  ,   7.980268  ,  -0.15842536], dtype=float32),\n",
       "  array([-25.538399  ,   7.9729266 ,  -0.16602755, -22.34692   ,\n",
       "           7.9431214 ,  -0.1350497 , -19.163757  ,   7.931518  ,\n",
       "          -0.08304587], dtype=float32)],\n",
       " [array([-20.775797 ,   7.9494977,  -0.1538507], dtype=float32),\n",
       "  array([-23.922638  ,   7.9321675 ,  -0.15201533, -20.748512  ,\n",
       "           7.908454  ,  -0.11630642, -17.576405  ,   7.90395   ,\n",
       "          -0.06144285], dtype=float32)],\n",
       " [array([-19.188837  ,   7.920091  ,  -0.14703375], dtype=float32),\n",
       "  array([-22.31312   ,   7.890009  ,  -0.13312185, -19.156582  ,\n",
       "           7.873928  ,  -0.09363043, -15.995365  ,   7.8777857 ,\n",
       "          -0.03666022], dtype=float32)],\n",
       " [array([-17.607576 ,   7.892531 ,  -0.1377995], dtype=float32),\n",
       "  array([-2.07109871e+01,  7.84739351e+00, -1.10640466e-01, -1.75719242e+01,\n",
       "          7.84007454e+00, -6.81046247e-02, -1.44213028e+01,  7.85321665e+00,\n",
       "         -9.60582495e-03], dtype=float32)],\n",
       " [array([-16.03159   ,   7.867321  ,  -0.12605162], dtype=float32),\n",
       "  array([-1.9117067e+01,  7.8051887e+00, -8.5820615e-02, -1.5995052e+01,\n",
       "          7.8073654e+00, -4.0771842e-02, -1.2854600e+01,  7.8303890e+00,\n",
       "          1.8846899e-02], dtype=float32)],\n",
       " [array([-14.460362  ,   7.8449593 ,  -0.11180731], dtype=float32),\n",
       "  array([-1.75316772e+01,  7.76413345e+00, -5.92207909e-02, -1.44260111e+01,\n",
       "          7.77635956e+00, -1.21036768e-02, -1.12951145e+01,  7.80970716e+00,\n",
       "          4.82997894e-02], dtype=float32)],\n",
       " [array([-12.893274  ,   7.825918  ,  -0.09520522], dtype=float32),\n",
       "  array([-15.959045  ,   7.728193  ,  -0.03084123, -12.867586  ,\n",
       "           7.750967  ,   0.01787639,  -9.74399   ,   7.794998  ,\n",
       "           0.07870471], dtype=float32)],\n",
       " [array([-11.329633  ,   7.810504  ,  -0.07707256], dtype=float32),\n",
       "  array([-1.4396995e+01,  7.6977878e+00, -3.2753944e-03, -1.1317394e+01,\n",
       "          7.7308111e+00,  4.6997070e-02, -8.1992006e+00,  7.7852468e+00,\n",
       "          1.0824427e-01], dtype=float32)],\n",
       " [array([-9.768688  ,  7.79894   , -0.05781847], dtype=float32),\n",
       "  array([-12.843019  ,   7.6718807 ,   0.02078164,  -9.773331  ,\n",
       "           7.714038  ,   0.07303882,  -6.659564  ,   7.7779617 ,\n",
       "           0.13509265], dtype=float32)],\n",
       " [array([-8.209647  ,  7.791471  , -0.03734493], dtype=float32),\n",
       "  array([-11.295309  ,   7.650183  ,   0.03971648,  -8.233734  ,\n",
       "           7.6998644 ,   0.094684  ,  -5.123814  ,   7.7719626 ,\n",
       "           0.15817523], dtype=float32)],\n",
       " [array([-6.6516685 ,  7.7883143 , -0.01578373], dtype=float32),\n",
       "  array([-9.730709  ,  7.626172  ,  0.0222975 , -6.678178  ,  7.672226  ,\n",
       "          0.08620811, -3.5798979 ,  7.743609  ,  0.15629745], dtype=float32)],\n",
       " [array([-5.0938716e+00,  7.7896547e+00,  6.7020166e-03], dtype=float32),\n",
       "  array([-8.1708145 ,  7.6319876 , -0.00921571, -5.114037  ,  7.66993   ,\n",
       "          0.06497896, -2.0193906 ,  7.7369246 ,  0.14299956], dtype=float32)],\n",
       " [array([-3.5353422 ,  7.7956386 ,  0.02992027], dtype=float32),\n",
       "  array([-6.6206646 ,  7.6579533 , -0.01867676, -3.5503697 ,  7.694495  ,\n",
       "          0.06124985, -0.44814688,  7.762432  ,  0.14360172], dtype=float32)],\n",
       " [array([-1.9751413 ,  7.80637   ,  0.05365403], dtype=float32),\n",
       "  array([-5.062315  ,  7.672954  , -0.01851714, -1.9826841 ,  7.7111864 ,\n",
       "          0.06565285,  1.1245508 ,  7.782636  ,  0.15109187], dtype=float32)],\n",
       " [array([-0.412314  ,  7.8219028 ,  0.07766683], dtype=float32),\n",
       "  array([-3.5005937 ,  7.6855073 , -0.01510799, -0.41284445,  7.7264304 ,\n",
       "          0.07290113,  2.6992307 ,  7.802195  ,  0.16105902], dtype=float32)],\n",
       " [array([1.1541008 , 7.842245  , 0.10170949], dtype=float32),\n",
       "  array([-1.9393191 ,  7.701886  , -0.01434815,  1.1576378 ,  7.744607  ,\n",
       "          0.07805187,  4.275832  ,  7.8239093 ,  0.16934836], dtype=float32)],\n",
       " [array([2.7250602 , 7.8673506 , 0.12552767], dtype=float32),\n",
       "  array([-0.38109696,  7.725197  , -0.02141988,  2.7272975 ,  7.7671857 ,\n",
       "          0.07677501,  5.853177  ,  7.847907  ,  0.17233372], dtype=float32)],\n",
       " [array([4.30151   , 7.897144  , 0.14896686], dtype=float32),\n",
       "  array([ 1.1732761 ,  7.7569304 , -0.04052746,  4.2958927 ,  7.7943454 ,\n",
       "          0.06556332,  7.430833  ,  7.8733015 ,  0.16708714], dtype=float32)],\n",
       " [array([5.884378 , 7.9315395, 0.1719793], dtype=float32),\n",
       "  array([ 2.7255697 ,  7.797778  , -0.07468587,  5.8654613 ,  7.8258424 ,\n",
       "          0.04190755,  9.010536  ,  7.899101  ,  0.15152028], dtype=float32)],\n",
       " [array([7.4745774 , 7.9704533 , 0.19456732], dtype=float32),\n",
       "  array([ 4.2799525e+00,  7.8478541e+00, -1.2559724e-01,  7.4402370e+00,\n",
       "          7.8612733e+00,  4.3995380e-03,  1.0596238e+01,  7.9244895e+00,\n",
       "          1.2446809e-01], dtype=float32)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test function ##\n",
    "def test(self,test_keys):\n",
    "    test_bar=pyprind.ProgBar(len(test_keys))\n",
    "    self.comparision_list=[]\n",
    "    #self.load_model_weights(\"Rollout_10\")\n",
    "    #print(\"loaded model weights\")\n",
    "        \n",
    "    self.test_run_comparision=[]\n",
    "    for test_run in range(len(test_keys)):\n",
    "        test_bar.update()\n",
    "        #key_test=str(test_keys[test_run])\n",
    "        #key_test=str(keys_for_training[test_run])\n",
    "        key_test=keys_for_training[0]\n",
    "        \n",
    "        ############# locally restrict the data for the training - we are restricting the number of timesteps to num_timesteps######\n",
    "        negative_vals=[]\n",
    "        for i in [0,1]:\n",
    "            negative_vals.append(list(filter(lambda x: x < 1, data_irl[key_test][i])))\n",
    "\n",
    "        num_timesteps=min(self.min_timesteps,len(negative_vals[0]),len(negative_vals[1]))\n",
    "        \n",
    "        # Ais initialization\n",
    "        time_step_loss=0\n",
    "        ais_rollout_enc=0.5*torch.ones(self.n_state_enc,dtype=torch.float).to(self.device)\n",
    "        action_list=[0,0]             \n",
    "\n",
    "        time_step_comparision=[]\n",
    "        for time_step in range(num_timesteps):                    \n",
    "            observation_list=[]\n",
    "            for i in range(len(data_irl[key_test])-2): # exclude the last two columns (actions/inputs)\n",
    "                temp=data_irl[key_test][i][time_step]\n",
    "                if temp!= None:\n",
    "                    observation_list.append(temp)\n",
    "                \n",
    "            input_list=[]\n",
    "            input_list.extend(observation_list)\n",
    "            input_list.extend(action_list)\n",
    "            input_tensor=torch.tensor(input_list,dtype=torch.float).to(self.device)\n",
    "            print(\"\\n\")\n",
    "            print(\"Before rollout, the input is \",input_tensor)\n",
    "            # Update action list for the next time step and for the reference\n",
    "            for i in [4,5]:\n",
    "                temp=data_irl[key_test][i][time_step]\n",
    "                if temp!= None:\n",
    "                    action_list[i-4]=temp\n",
    "                else:\n",
    "                    action_list[i-4]=0\n",
    "\n",
    "            reference_list=[]\n",
    "            observation_list_ref=[]\n",
    "            for i in [1,3]:#range(len(data_irl[key_test])-2): # exclude the last two columns (actions/inputs)\n",
    "                temp=data_irl[key_test][i][time_step+1]\n",
    "                if temp!= None:\n",
    "                    observation_list_ref.append(temp)                      \n",
    "            reference_list.extend(observation_list_ref)\n",
    "            reference_list.extend([action_list[1]])\n",
    "\n",
    "            reference_tensor=1*torch.tensor(reference_list,dtype=torch.float).to(self.device)\n",
    "\n",
    "            ais_rollout_dec=0.5*torch.ones(self.n_state_dec,dtype=torch.float).to(self.device) # Just initialize it to 0.5\n",
    "\n",
    "\n",
    "            rollout_reference=0.5*torch.ones(self.n_output,dtype=torch.float).to(self.device)\n",
    "\n",
    "            rollout_prediction=0.5*torch.ones(self.n_output,dtype=torch.float).to(self.device)\n",
    "\n",
    "            #internal_rollout_horizon=min(self.rollout,len(data_irl[key_test][0])-time_step-2)\n",
    "            internal_rollout_horizon=1\n",
    "            rollout_loss=0\n",
    "\n",
    "            rollout_comparison=[]\n",
    "            for k in range(internal_rollout_horizon):\n",
    "                temp_comparision=[]\n",
    "                \n",
    "                #print(\"\\n\")\n",
    "                if k==0:\n",
    "                    rollout_input=input_tensor\n",
    "                    rollout_reference=reference_tensor\n",
    "                    #ais_rollout_enc=ais\n",
    "                    print(\"for rollout\",k,\"ais \",ais_rollout_enc)\n",
    "\n",
    "                    ais_rollout_enc=self.gen_model(rollout_input,ais_rollout_enc)\n",
    "                    #ais=ais_rollout_enc\n",
    "                    \n",
    "                    #decoder_input=list(ais_rollout_enc.detach().cpu().numpy())\n",
    "                    #decoder_input.extend([time_step+k])\n",
    "                    #decoder_input_tensor=torch.tensor(decoder_input,dtype=torch.float).to(self.device)\n",
    "                    \n",
    "                    time_step_tensor=torch.tensor([time_step+k],dtype=torch.float).to(self.device)\n",
    "                    decoder_input_tensor=torch.cat((ais_rollout_enc,time_step_tensor),0)\n",
    "                    rollout_prediction,ais_rollout_dec=self.pred_model(decoder_input_tensor,ais_rollout_dec)\n",
    "                    \n",
    "                    print(\"for rollout\",k,\"rollout_reference is \",rollout_reference)\n",
    "                    print(\"for rollout\",k,\"decoder_input  is \",decoder_input_tensor,time_step+k)\n",
    "                    print(\"for rollout\",k,\"rollout_prediction is \",rollout_prediction)\n",
    "                    \n",
    "                    rollout_loss+=-torch.distributions.MultivariateNormal(rollout_prediction,torch.eye(rollout_prediction.shape[0]).to(self.device)).log_prob(rollout_reference)\n",
    "\n",
    "                    temp_comparision.extend(rollout_reference.detach().cpu().numpy())\n",
    "                    temp_comparision.extend(rollout_prediction.detach().cpu().numpy())\n",
    "\n",
    "                else:\n",
    "                    rollout_reference_list=[]                           \n",
    "                    for i in [1,3,5]:#range(len(data_irl[key_test])):\n",
    "                        if i<4:\n",
    "                            rollout_reference_list.append(data_irl[key_test][i][time_step+k+1])\n",
    "                        elif i>=4 and data_irl[key_test][i][time_step+k]!=None: # Previous action is observed at time_step+k+1\n",
    "                            rollout_reference_list.append(data_irl[key_test][i][time_step+k])\n",
    "                        else:\n",
    "                            rollout_reference_list.append(0) # Zero padding for the input action\n",
    "\n",
    "                    rollout_reference=1*torch.tensor(rollout_reference_list,dtype=torch.float).to(self.device)\n",
    "\n",
    "                    \n",
    "                    #decoder_input=list(ais_rollout_enc.detach().cpu().numpy())\n",
    "                    #decoder_input.extend([time_step+k])\n",
    "                    #decoder_input_tensor=torch.tensor(decoder_input,dtype=torch.float).to(self.device)\n",
    "\n",
    "                    time_step_tensor=torch.tensor([time_step+k],dtype=torch.float).to(self.device)\n",
    "                    decoder_input_tensor=torch.cat((ais_rollout_enc,time_step_tensor),0)\n",
    "                    rollout_prediction,ais_rollout_dec=self.pred_model(decoder_input_tensor,ais_rollout_dec)                                                \n",
    "                    \n",
    "                    print(\"for rollout\",k,\"rollout_reference is \",rollout_reference)\n",
    "                    print(\"for rollout\",k,\"decoder_input  is \",decoder_input_tensor,time_step+k)\n",
    "                    print(\"for rollout\",k,\"rollout_prediction is \",rollout_prediction)\n",
    "\n",
    "                    temp_comparision.extend(rollout_reference.detach().cpu().numpy())\n",
    "                    temp_comparision.extend(rollout_prediction.detach().cpu().numpy())\n",
    "                    rollout_loss+=-torch.distributions.MultivariateNormal(rollout_prediction,torch.eye(rollout_prediction.shape[0]).to(self.device)).log_prob(rollout_reference)\n",
    "                rollout_comparison.append(temp_comparision)\n",
    "            time_step_comparision.append(rollout_comparison)    \n",
    "        self.test_run_comparision.extend(time_step_comparision)\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3360/1318344120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_run_comparision\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     '''\n\u001b[0;32m     25\u001b[0m     \u001b[0mpos_tref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_run_comparision\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#print(len(test_results.test_run_comparision[0][1]))\n",
    "\n",
    "#test_results.test_run_comparision[19][:]\n",
    "\n",
    "\n",
    "# Plot the results only for the HDV\n",
    "pos_t=[]\n",
    "vel_t=[]\n",
    "acc_t=[]\n",
    "pos_tref=[]\n",
    "vel_tref=[]\n",
    "acc_tref=[]\n",
    "pos_t_hdv=[]\n",
    "vel_t_hdv=[]\n",
    "acc_t_hdv=[]\n",
    "pos_tref_hdv=[]\n",
    "vel_tref_hdv=[]\n",
    "acc_tref_hdv=[]\n",
    "\n",
    "# set which timestep to plot\n",
    "time_step=10\n",
    "\n",
    "for index in range(len(test_results.test_run_comparision[time_step])):\n",
    "    '''\n",
    "    pos_tref.append(test_results.test_run_comparision[time_step][index][0])\n",
    "    vel_tref.append(test_results.test_run_comparision[time_step][index][2])\n",
    "    acc_tref.append(test_results.test_run_comparision[time_step][index][4])\n",
    "    pos_t.append(test_results.test_run_comparision[time_step][index][6])\n",
    "    vel_t.append(test_results.test_run_comparision[time_step][index][8])\n",
    "    acc_t.append(test_results.test_run_comparision[time_step][index][10])\n",
    "    \n",
    "    #pos_t_hdv.append(test_results.test_run_comparision[time_step][index][1])\n",
    "    #vel_t_hdv.append(test_results.test_run_comparision[time_step][index][3])\n",
    "    #acc_t_hdv.append(test_results.test_run_comparision[time_step][index][5])\n",
    "    #pos_tref_hdv.append(test_results.test_run_comparision[time_step][index][7])\n",
    "    #vel_tref_hdv.append(test_results.test_run_comparision[time_step][index][9])\n",
    "    #acc_tref_hdv.append(test_results.test_run_comparision[time_step][index][11])\n",
    "\n",
    "    \n",
    "    pos_tref_hdv.append(test_results.test_run_comparision[time_step][index][1])\n",
    "    vel_tref_hdv.append(test_results.test_run_comparision[time_step][index][3])\n",
    "    acc_tref_hdv.append(test_results.test_run_comparision[time_step][index][5])\n",
    "    pos_t_hdv.append(test_results.test_run_comparision[time_step][index][7])\n",
    "    vel_t_hdv.append(test_results.test_run_comparision[time_step][index][9])\n",
    "    acc_t_hdv.append(test_results.test_run_comparision[time_step][index][11])\n",
    "    '''\n",
    "\n",
    "    pos_tref_hdv.append(test_results.test_run_comparision[time_step][index][0])\n",
    "    vel_tref_hdv.append(test_results.test_run_comparision[time_step][index][1])\n",
    "    acc_tref_hdv.append(test_results.test_run_comparision[time_step][index][2])\n",
    "    pos_t_hdv.append(test_results.test_run_comparision[time_step][index][3])\n",
    "    vel_t_hdv.append(test_results.test_run_comparision[time_step][index][4])\n",
    "    acc_t_hdv.append(test_results.test_run_comparision[time_step][index][5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data=data_irl[str(keys_for_testing[10])]    \n",
    "\n",
    "pos_t_hdv=[]\n",
    "vel_t_hdv=[]\n",
    "acc_t_hdv=[]\n",
    "pos_tref_hdv=[]\n",
    "vel_tref_hdv=[]\n",
    "acc_tref_hdv=[]\n",
    "\n",
    "    for index in range(len(plot_data[0])):\n",
    "\n",
    "        pos_t_hdv.append(test_results.test_run_comparision[time_step][index][1].detach().cpu().numpy())\n",
    "        vel_t_hdv.append(test_results.test_run_comparision[time_step][index][3].detach().cpu().numpy())\n",
    "        acc_t_hdv.append(test_results.test_run_comparision[time_step][index][5].detach().cpu().numpy())\n",
    "        pos_tref_hdv.append(test_results.test_run_comparision[time_step][index][7].detach().cpu().numpy())\n",
    "        vel_tref_hdv.append(test_results.test_run_comparision[time_step][index][9].detach().cpu().numpy())\n",
    "        acc_tref_hdv.append(test_results.test_run_comparision[time_step][index][11].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting outputs #\n",
    "import matplotlib.colors as mcolors\n",
    "####### For X - lateral#########\n",
    "fig, ax = plt.subplots()\n",
    "#plt.plot(y_ref,x_ref,'r',linewidth=2.0,label=\"Actual\")\n",
    "#plt.plot(y_pred,x_pred,'b',linewidth=2.0,label=\"Predicted\")\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(pos_t_hdv,mcolors.CSS4_COLORS['salmon'],linewidth=2.0,label=\"Actual\")\n",
    "plt.plot(pos_tref_hdv,mcolors.CSS4_COLORS['limegreen'],linewidth=2.0,label=\"Predicted\")\n",
    "plt.xlabel('Time in (1/10)s')\n",
    "plt.ylabel('Position along the road m')\n",
    "plt.title('Comparision_of_position_along_road')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('graphs/irl_graphs/Comparision_rollout10_timestep_min20_posref_pospred_g1_p2_lr0005.png')\n",
    "plt.close()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(vel_t_hdv,mcolors.CSS4_COLORS['salmon'],linewidth=2.0,label=\"Actual\")\n",
    "plt.plot(vel_tref_hdv,mcolors.CSS4_COLORS['limegreen'],linewidth=2.0,label=\"Predicted\")\n",
    "plt.xlabel('Time in (1/10)s')\n",
    "plt.ylabel('Velocity along the road in m/s')\n",
    "plt.title('Comparision_Lateral_Velocities')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('graphs/irl_graphs/Comparision_rollout10_timestep_min20_vref_vpre_g1_p2_lr0005.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-46.4000, device='cuda:0'),\n",
       " tensor(-53.2063, device='cuda:0'),\n",
       " tensor(8., device='cuda:0'),\n",
       " tensor(7.4680, device='cuda:0'),\n",
       " tensor(-5., device='cuda:0'),\n",
       " tensor(-1.3502, device='cuda:0'),\n",
       " tensor(-31.8437, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
       " tensor(-27.8377, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
       " tensor(4.1773, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
       " tensor(9.0293, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
       " tensor(-1.2700, device='cuda:0', grad_fn=<UnbindBackward0>),\n",
       " tensor(0.0087, device='cuda:0', grad_fn=<UnbindBackward0>)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.test_run_comparision[1][0]\n",
    "#len(test_results.test_run_comparision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = torch.nn.functional.nll_loss(torch.nn.LogSoftmax(input, dim=1), target)\n",
    "output.backward()\n",
    "'''\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output2 = torch.nn.functional.nll_loss(m(input), target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19aadfa955e96a5e7fdf9b64cb086b0ad6aa114b0d422a86116b9caa90456979"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
